{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576c2918",
   "metadata": {},
   "source": [
    "# How to: Custom Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee88ccb",
   "metadata": {},
   "source": [
    "It is possible to extend deepchecks by implementing custom check. This is desirable when you want to have your own logic of metrics or validation, or even just to display your own graph using deepchecks' suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258972c9",
   "metadata": {},
   "source": [
    "This guide will walk you through creating a check with your own logic.\n",
    "Each check consists of 3 main parts: return value (mandatory), display and conditions. we will implement here the return value and display, for conditions see [link how to create conditions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf03655",
   "metadata": {},
   "source": [
    "## Basic Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967574aa",
   "metadata": {},
   "source": [
    "Let's implement a check that will compare the test and train datasets sizes.\n",
    "The first step is to create check class, which inherits from a base check class. each base check is differed by it's run method signature or by the purpose it fulfills [link to all]. in this case we will use `TrainTestBaseCheck`, which used to compare between test and train datasets. After creating the basic class with the `run` function we will write our check logic inside it.\n",
    "\n",
    "*Good to know: the return value of a check can be any object, a number, dictionary, string, etc...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804b2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks import TrainTestBaseCheck, CheckResult, Dataset\n",
    "\n",
    "\n",
    "class DatasetSizeComparison(TrainTestBaseCheck):\n",
    "    \"\"\"Check which compares the sizes of train and test datasets.\"\"\"\n",
    "    \n",
    "    def run(self, train_dataset: Dataset, test_dataset: Dataset, model=None) -> CheckResult:\n",
    "        ## Check logic\n",
    "        train_size = train_dataset.n_samples\n",
    "        test_size = test_dataset.n_samples\n",
    "        \n",
    "        ## Return value as check result\n",
    "        return_value = {'train_size': train_size, 'test_size': test_size}\n",
    "        return CheckResult(return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fea87e",
   "metadata": {},
   "source": [
    "Hooray! we just wrote our custom check. now let's create 2 datasets and try to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c3e4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Dataset Size Comparison</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Check which compares the sizes of train and test datasets.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>&#x2713;</b> Nothing found</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO which data to load\n",
    "train_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}))\n",
    "test_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}))\n",
    "\n",
    "DatasetSizeComparison().run(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1543a8",
   "metadata": {},
   "source": [
    "Our check run successfully but we got the print \"Nothing found\". this is because we haven't defined to the check anything to display, so the default behavior is to print \"Nothing found\". in order to access the value we defined earlier we can use the \"value\" property on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffdbf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_size': 9, 'test_size': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = DatasetSizeComparison().run(train_dataset, test_dataset)\n",
    "result.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0845a6",
   "metadata": {},
   "source": [
    "## Check Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97d44b",
   "metadata": {},
   "source": [
    "Most of the times we will want our checks to have a visual display that will quickly summarize the check result. we can pass to the `CheckResult` objects to be displayed like html, dataframe or graph. so let's define a graph that will be displayed using `matplotlib`. in order to use `matplotlib` we have to implement the code inside a function and not call it directly in the check, this is due to architecture limitations of `matplotlib` (read more here...)\n",
    "\n",
    "*Good to know: `display` can receive a single object to display or a list of objects*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d0af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks import TrainTestBaseCheck, CheckResult, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DatasetSizeComparison(TrainTestBaseCheck):\n",
    "    \"\"\"Check which compares the sizes of train and test datasets.\"\"\"\n",
    "    \n",
    "    def run(self, train_dataset: Dataset, test_dataset: Dataset, model=None) -> CheckResult:\n",
    "        ## Check logic\n",
    "        train_size = train_dataset.n_samples\n",
    "        test_size = test_dataset.n_samples\n",
    "        \n",
    "        ## Create the check result value\n",
    "        sizes = {'Train': train_size, 'Test': test_size}\n",
    "        \n",
    "        ## Display function of matplotlib graph:\n",
    "        def graph_display():\n",
    "            plt.bar(sizes.keys(), sizes.values(), color='green')\n",
    "            plt.xlabel(\"Dataset\")\n",
    "            plt.ylabel(\"Size\")\n",
    "            plt.title(\"Datasets size comparison\")\n",
    "        \n",
    "        return CheckResult(sizes, display=graph_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575051be",
   "metadata": {},
   "source": [
    "Let check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c146a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Dataset Size Comparison</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Check which compares the sizes of train and test datasets.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DatasetSizeComparison().run(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd3515",
   "metadata": {},
   "source": [
    "Voila! now we have a check that prints a graph and have a value. we can add this check to any suite and it will run with it.  \n",
    "The next possible step is to implement condition, which will allow us to give a check result a score of pass / not pass. to do it check out the following guide [link]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
