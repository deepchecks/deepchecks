"""Contains unit tests for the performance overfit check."""
import typing as t
import re
from numbers import Number

import pandas as pd

from deepchecks import Dataset
from deepchecks.base.check import Condition
from deepchecks.base.check import ConditionResult
from deepchecks.base.check import ConditionCategory
from deepchecks.checks import TrainTestDifferenceOverfit
from deepchecks.utils import DeepchecksValueError
from deepchecks.metric_utils import DEFAULT_MULTICLASS_METRICS
from hamcrest import assert_that, calling, raises, close_to, starts_with, has_items

from tests.checks.utils import equal_condition_result
from tests.checks.utils import ANY_FLOAT_REGEXP


def test_dataset_wrong_input():
    bad_dataset = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(bad_dataset, None, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires dataset to be of type Dataset. instead '
                       'got: str'))


def test_model_wrong_input(iris_labeled_dataset):
    bad_model = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_labeled_dataset,
                                                                    bad_model),
                raises(DeepchecksValueError,
                       'Model must inherit from one of supported models: .*'))


def test_dataset_no_label(iris_dataset):
    # Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_dataset, iris_dataset, None),
                raises(DeepchecksValueError, 'Check TrainTestDifferenceOverfit requires dataset to have a '
                                           'label column'))


def test_dataset_no_shared_label(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(iris_labeled_dataset.data, label='sepal length (cm)')
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same label'))


def test_dataset_no_shared_features(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(pd.concat(
        [iris_labeled_dataset.data,
         iris_labeled_dataset.data[['sepal length (cm)']].rename(columns={'sepal length (cm)': '1'})],
        axis=1),
        label=iris_labeled_dataset.label_name())
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same features'))


def test_no_diff(iris_split_dataset_and_model):
    # Arrange
    train, _, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, train, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(0, 0.001))


def test_with_diff(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(-0.035, 0.01))


def test_custom_metrics(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit(
        alternative_metrics={'Accuracy': 'accuracy', 'Always 0.5': lambda x, y, z: 0.5}
    )
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert isinstance(value, Number)


def test_train_test_difference_condition_that_should_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_difference_not_greater_than` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
    })
    validate_train_test_condition(
        df=condition_satisfying_df,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        expecting_to_pass=True,
    )


def test_train_test_difference_condition_that_should_not_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_difference_not_greater_than` method
    """
    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # all test metrics do not satisfy condition
        'Test Metrics': {'x1': 0.5, 'x2': 0.2, 'x3': 0.3},
    })
    validate_train_test_condition(
        df=condition_unsatisfying_df,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        expecting_to_pass=False,
        failure_message='cond var:{var}, all metrics:{all_metric_values}, failed metrics:{failed_metric_values}.',
        details_pattern=re.compile(
            fr'^cond var:{ANY_FLOAT_REGEXP.pattern}, '
            fr'all metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}, '
            fr'failed metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}\.$'
        )
    )


def test_train_test_ratio_condition_that_should_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_ratio_not_greater_than` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.8, 'x2': 0.61, 'x3': 0.71},
    })
    validate_train_test_condition(
        df=condition_satisfying_df,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        expecting_to_pass=True,
    )


def test_train_test_ratio_condition_that_should_not_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_ratio_not_greater_than` method
    """
    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # all test metrics do not satisfy condition
        'Test Metrics': {'x1': 0.41, 'x2': 0.3, 'x3': 0.3},
    })
    validate_train_test_condition(
        df=condition_unsatisfying_df,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        expecting_to_pass=False,
        failure_message='cond var:{var}, all metrics:{all_metric_values}, failed metrics:{failed_metric_values}.',
        details_pattern=re.compile(
            fr'^cond var:{ANY_FLOAT_REGEXP.pattern}, '
            fr'all metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}, '
            fr'failed metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}\.$'
        )
    )


def test_train_test_difference_condition_for_specified_metrics():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.train_test_difference_not_greater_than` method
    """

    df_with_unsatisfying_x1_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x1 metric does not satisfy condition
        'Test Metrics': {'x1': 0.5, 'x2': 0.64, 'x3': 0.71},
    })

    df_with_unsatisfying_x2_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x2 metric does not satisfy condition
        'Test Metrics': {'x1': 0.79, 'x2': 0.17, 'x3': 0.71},
    })

    # condition for x1 metric should not fail for df with wrong x2 but correct x1 metric
    validate_train_test_condition(
        df=df_with_unsatisfying_x2_metric_condition,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        metrics=["x1"],
        expecting_to_pass=True,
    )

    # condition for x1 metric should fail for df with wrong x1
    validate_train_test_condition(
        df=df_with_unsatisfying_x1_metric_condition,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        metrics=["x1"],
        expecting_to_pass=False,
        failure_message='cond var:{var}, all metrics:{all_metric_values}, failed metrics:{failed_metric_values}.',
        details_pattern=re.compile(
            fr'^cond var:{ANY_FLOAT_REGEXP.pattern}, '
            fr'all metrics:x1={ANY_FLOAT_REGEXP.pattern}, '
            fr'failed metrics:x1={ANY_FLOAT_REGEXP.pattern}\.$'
        )
    )



def test_train_test_ratio_condition_for_specified_metrics():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_ratio_not_greater_than` method
    """

    df_with_unsatisfying_x1_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x1 metric does not satisfy condition
        'Test Metrics': {'x1': 0.41, 'x2': 0.64, 'x3': 0.71},
    })

    df_with_unsatisfying_x2_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x2 metric does not satisfy condition
        'Test Metrics': {'x1': 0.79, 'x2': 0.3, 'x3': 0.71},
    })

    # condition for x1 metric should not fail for df with wrong x2 but correct x1 metric
    validate_train_test_condition(
        df=df_with_unsatisfying_x2_metric_condition,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        metrics=["x1"],
        expecting_to_pass=True,
    )

    # condition for x1 metric should fail for df with wrong x1
    validate_train_test_condition(
        df=df_with_unsatisfying_x1_metric_condition,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        metrics=["x1"],
        expecting_to_pass=False,
        failure_message='cond var:{var}, all metrics:{all_metric_values}, failed metrics:{failed_metric_values}.',
        details_pattern=re.compile(
            fr'^cond var:{ANY_FLOAT_REGEXP.pattern}, '
            fr'all metrics:x1={ANY_FLOAT_REGEXP.pattern}, '
            fr'failed metrics:x1={ANY_FLOAT_REGEXP.pattern}\.$'
        )
    )


try:
    # NOTE: python 3.6 does not support t.Literal
    TrainDifferenceCondition = t.Union[
        t.Literal["train_test_difference_greater_than"],
        t.Literal["train_test_ratio_greater_than"]
    ]
except:
    TrainDifferenceCondition = str


def validate_train_test_condition(
    *,
    df: pd.DataFrame,
    condition_type: TrainDifferenceCondition,
    condition_value: float,
    metrics: t.Optional[t.Sequence[str]] = None,
    name: str = "Test Condition",
    category: ConditionCategory = ConditionCategory.WARN,
    failure_message: str = "Condition Failed",
    expecting_to_pass: bool = True,
    details_pattern: t.Pattern = re.compile(r'.*'),
    assert_description: t.Optional[str] = None
):
    # Check that dataframe has correct schema for the testing purpose

    if not all(df.columns == ["Training Metrics", "Test Metrics"]):
        raise ValueError("incorrect df schema")

    if not all(df["Training Metrics"].index == df["Test Metrics"].index):
        raise ValueError("incorrect df schema")
    
    check = TrainTestDifferenceOverfit()
    method_name = f'add_condition_{condition_type}'
    
    if not hasattr(check, method_name):
        raise ValueError(f'Unknown method: {method_name}')
    else:
        add_condition = getattr(check, method_name)

    # NOTE: we are accessing check instance private field `_conditions`
    # in order to work with generated conditions directly

    condition, *_ = t.cast(
        t.List[Condition],
        list(add_condition(
            var=condition_value,
            metrics=metrics,
            name=name,
            category=category,
            failure_message=failure_message
        )._conditions.values())
    )

    result = t.cast(ConditionResult, condition(df))
    
    assert_that(
        result, 
        matcher=equal_condition_result( # type: ignore
            is_pass=expecting_to_pass,
            name=name,
            details=details_pattern,
            category=category
        ),
        reason=assert_description or ""
    )