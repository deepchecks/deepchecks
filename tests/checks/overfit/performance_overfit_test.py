"""Contains unit tests for the performance overfit check."""
import typing as t
import re
from numbers import Number

import pandas as pd

from deepchecks import Dataset
from deepchecks.base.check import Condition
from deepchecks.base.check import ConditionResult
from deepchecks.base.check import ConditionCategory
from deepchecks.checks import TrainTestDifferenceOverfit
from deepchecks.utils import DeepchecksValueError
from deepchecks.metric_utils import DEFAULT_MULTICLASS_METRICS
from hamcrest import assert_that, calling, raises, close_to, starts_with, has_items

from tests.checks.utils import equal_condition_result
from tests.checks.utils import ANY_FLOAT_REGEXP


def test_dataset_wrong_input():
    bad_dataset = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(bad_dataset, None, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires dataset to be of type Dataset. instead '
                       'got: str'))


def test_model_wrong_input(iris_labeled_dataset):
    bad_model = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_labeled_dataset,
                                                                    bad_model),
                raises(DeepchecksValueError,
                       'Model must inherit from one of supported models: .*'))


def test_dataset_no_label(iris_dataset):
    # Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_dataset, iris_dataset, None),
                raises(DeepchecksValueError, 'Check TrainTestDifferenceOverfit requires dataset to have a '
                                           'label column'))


def test_dataset_no_shared_label(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(iris_labeled_dataset.data, label='sepal length (cm)')
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same label'))


def test_dataset_no_shared_features(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(pd.concat(
        [iris_labeled_dataset.data,
         iris_labeled_dataset.data[['sepal length (cm)']].rename(columns={'sepal length (cm)': '1'})],
        axis=1),
        label=iris_labeled_dataset.label_name())
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same features'))


def test_no_diff(iris_split_dataset_and_model):
    # Arrange
    train, _, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, train, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(0, 0.001))


def test_with_diff(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(-0.035, 0.01))


def test_custom_metrics(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit(
        alternative_metrics={'Accuracy': 'accuracy', 'Always 0.5': lambda x, y, z: 0.5}
    )
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert isinstance(value, Number)


def test_train_difference_condition():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_is_lower_by` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
    })

    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.5, 'x2': 0.2, 'x3': 0.3},
    })

    validate_train_difference_condition(
        condition_satisfying_df,
        condition_unsatisfying_df,
        condition_value=0.2,
        condition_generating_method="train_is_lower_by"
    )


def test_train_difference_by_factor_of_X_condition():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_is_lower_by_factor_of` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.8, 'x2': 0.61, 'x3': 0.71},
    })

    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.41, 'x2': 0.3, 'x3': 0.3},
    })

    validate_train_difference_condition(
        condition_satisfying_df,
        condition_unsatisfying_df,
        condition_value=2,
        condition_generating_method="train_is_lower_by_factor_of"
    )


def test_train_difference_condition_for_specified_metrics():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_is_lower_by` method
    """

    df_with_unsatisfying_x1_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x1 metric does not satisfy condition
        'Test Metrics': {'x1': 0.5, 'x2': 0.64, 'x3': 0.71},
    })

    df_with_unsatisfying_x2_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x2 metric does not satisfy condition
        'Test Metrics': {'x1': 0.79, 'x2': 0.17, 'x3': 0.71},
    })

    validate_train_difference_condition_for_specified_metrics(
        df_with_unsatisfying_x1_metric_condition,
        df_with_unsatisfying_x2_metric_condition,
        condition_value=0.2,
        condition_generating_method="train_is_lower_by"
    )


def test_train_difference_by_factor_of_X_condition_for_specified_metrics():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_is_lower_by_factor_of` method
    """

    df_with_unsatisfying_x1_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x1 metric does not satisfy condition
        'Test Metrics': {'x1': 0.41, 'x2': 0.64, 'x3': 0.71},
    })

    df_with_unsatisfying_x2_metric_condition = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # x2 metric does not satisfy condition
        'Test Metrics': {'x1': 0.79, 'x2': 0.3, 'x3': 0.71},
    })

    validate_train_difference_condition_for_specified_metrics(
        df_with_unsatisfying_x1_metric_condition,
        df_with_unsatisfying_x2_metric_condition,
        condition_value=2,
        condition_generating_method="train_is_lower_by_factor_of"
    )


try:
    # NOTE: python 3.6 does not support t.Literal
    TrainDifferenceCondition = t.Union[
        t.Literal["train_is_lower_by"],
        t.Literal["train_is_lower_by_factor_of"]
    ]
except:
    TrainDifferenceCondition = str


def validate_train_difference_condition(
    condition_satisfying_df: pd.DataFrame,
    condition_unsatisfying_df: pd.DataFrame,
    condition_value: float,
    condition_generating_method: TrainDifferenceCondition,
):
    """
    General function to validate condition correctens generated by
    - `TrainTestDifferenceOverfit.add_condition_train_is_lower_by` method
    - `TrainTestDifferenceOverfit.add_condition_train_is_lower_by_factor_of` method

    in general it was added to reduce boilerplate

    Expecting dataframes with next schema:

    index | Training metrics | Test Metrics
    ---------------------------------------
      x1  |  <float value>   | <float value>
      x2  |  <float value>   | <float value>
      x3  |  <float value>   | <float value>

    Args:
        condition_satisfying_df: dataframe that satisfies condition
        condition_unsatisfying_df: dataframe that does not satisfy condition
        condition_value: ...
        condition_generating_method: one of `train_is_lower_by` or `train_is_lower_by_factor_of`

    It will validate that:
        - applying `condition_satisfying_df` to the condition returns `Not Passed`
        - applying `condition_unsatisfying_df` to the condition returns `Passed`
    """
    check = TrainTestDifferenceOverfit()

    if condition_generating_method == "train_is_lower_by_factor_of":
        add_condition = check.add_condition_train_is_lower_by_factor_of
    elif condition_generating_method == "train_is_lower_by":
        add_condition = check.add_condition_train_is_lower_by
    else:
        raise ValueError("Unkown method")

    # NOTE: we are accessing check instance private field `_conditions`
    # in order to work with generated conditions directly

    condition, *_ = t.cast(
        t.List[Condition],
        list(add_condition(
            var=condition_value,
            metrics=['x1', 'x2', 'x3'],
            name='Test Condition',
            category=ConditionCategory.WARN,
            failure_message='cond var:{var}, all metrics:{all_metric_values}, failed metrics:{failed_metric_values}.',
        )._conditions.values())
    )

    result = t.cast(ConditionResult, condition(condition_satisfying_df))
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=True,
        name='Test Condition',
        details='',
        category=ConditionCategory.WARN
    ))

    details_regexp = re.compile(
        fr'^cond var:{ANY_FLOAT_REGEXP.pattern}, '
        fr'all metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}, '
        fr'failed metrics:x1={ANY_FLOAT_REGEXP.pattern};x2={ANY_FLOAT_REGEXP.pattern};x3={ANY_FLOAT_REGEXP.pattern}.$'
    )

    result = t.cast(ConditionResult, condition(condition_unsatisfying_df))
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=False,
        name='Test Condition',
        details=details_regexp,
        category=ConditionCategory.WARN
    ))


def validate_train_difference_condition_for_specified_metrics(
    df_with_unsatisfying_x1_metric_condition: pd.DataFrame,
    df_with_unsatisfying_x2_metric_condition: pd.DataFrame,
    condition_value: float,
    condition_generating_method: TrainDifferenceCondition,
):
    """
    General function to validate condition correctens generated by
    - `TrainTestDifferenceOverfit.add_condition_train_is_lower_by` method
    - `TrainTestDifferenceOverfit.add_condition_train_is_lower_by_factor_of` method

    in general it was added to reduce boilerplate

    What does it doing:
        we have 2 dataframes (df1, df2) and 3 conditions (c1, c2, c3)

        c1 - tests that x1 metric is not lower by X (lower by a factor of X)
        c2 - tests that x2 metric is not lower by X (lower by a factor of X)
        c3 - tests that all metrics are not lower by X (lower by a factor of X)

        df1 - has x1 metric that should not satisfy c1
        df2 - has x2 metric that should not satisfy c2
        df1 and df2 - should not satisfy c3

        and we test that:
        - applying df1 to c1 returns Not Passed
        - applying df2 to c2 returns Not Passed
        - applying df1 to c2 returns Passed
        - applying df2 to c1 returns Passed
        - applying df1, df2 to c3 returns Not Passed

    Expecting dataframes with next schema:

    index | Training metrics | Test Metrics
    ---------------------------------------
      x1  |  <float value>   | <float value>
      x2  |  <float value>   | <float value>
      x3  |  <float value>   | <float value>

    Args:
        df_with_unsatisfying_x1_metric_condition: dataframe with x1 values that do not satisfy `condition_for_x1_metric`
        df_with_unsatisfying_x2_metric_condition: dataframe with x1 values that do not satisfy `condition_for_x2_metric`
        condition_value: ...
        condition_generating_method: one of `train_is_lower_by` or `train_is_lower_by_factor_of`

    """
    check = TrainTestDifferenceOverfit()

    if condition_generating_method == "train_is_lower_by_factor_of":
        add_condition = check.add_condition_train_is_lower_by_factor_of
    elif condition_generating_method == "train_is_lower_by":
        add_condition = check.add_condition_train_is_lower_by
    else:
        raise ValueError("Unkown method")

    # NOTE: we are accessing check instance private field `_conditions`
    # in order to work with generated conditions directly

    condition_for_x1_metric, *_ = t.cast(
        t.List[Condition],
        list(add_condition(
            var=condition_value,
            name='Condition for x1 metric',
            metrics='x1',
            failure_message='Condition failed'
        )._conditions.values())
    )

    # NOTE: clearing '_conditions' list after we obtained reference
    # to the generated condition instance
    check._conditions.clear()

    condition_for_x2_metric, *_ = t.cast(
        t.List[Condition],
        list(add_condition(
            var=condition_value,
            name="Condition for x2 metric",
            metrics='x2',
            failure_message='Condition failed'
        )._conditions.values())
    )

    check._conditions.clear()

    condition_for_all_metrics, *_ = t.cast(
        t.List[Condition],
        list(add_condition(
            var=condition_value,
            name='General Condition',
            failure_message='Condition failed'
        )._conditions.values())
    )

    check._conditions.clear()

    # assert that x1 df fails with x1 condition
    result = t.cast(
        ConditionResult,
        condition_for_x1_metric(df_with_unsatisfying_x1_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=False,
        name='Condition for x1 metric',
        details='Condition failed',
        category=ConditionCategory.FAIL
    ))

    # assert that x2 df fails with x2 condition
    result = t.cast(
        ConditionResult,
        condition_for_x2_metric(df_with_unsatisfying_x2_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=False,
        name='Condition for x2 metric',
        details='Condition failed',
        category=ConditionCategory.FAIL
    ))

    # assert that x2 df do not fail with x1 condition
    result = t.cast(
        ConditionResult,
        condition_for_x1_metric(df_with_unsatisfying_x2_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=True,
        name='Condition for x1 metric',
        details='',
        category=ConditionCategory.FAIL
    ))

    # assert that x1 df do not fail with x2 condition
    result = t.cast(
        ConditionResult,
        condition_for_x2_metric(df_with_unsatisfying_x1_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=True,
        name='Condition for x2 metric',
        details='',
        category=ConditionCategory.FAIL
    ))

    # assert that x1 df fails with general condtion (all features condition)
    result = t.cast(
        ConditionResult,
        condition_for_all_metrics(df_with_unsatisfying_x1_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=False,
        name='General Condition',
        details='Condition failed',
        category=ConditionCategory.FAIL
    ))

    # assert that x2 df fails with general condtion (all features condition)
    result = t.cast(
        ConditionResult,
        condition_for_all_metrics(df_with_unsatisfying_x2_metric_condition)
    )
    assert_that(result, matcher=equal_condition_result( # type: ignore
        is_pass=False,
        name='General Condition',
        details='Condition failed',
        category=ConditionCategory.FAIL
    ))