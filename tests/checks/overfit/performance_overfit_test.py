"""Contains unit tests for the performance overfit check."""
import typing as t
import re
from numbers import Number

import pandas as pd

from deepchecks import Dataset
from deepchecks.base.check import Condition
from deepchecks.base.check import ConditionResult
from deepchecks.base.check import ConditionCategory
from deepchecks.checks import TrainTestDifferenceOverfit
from deepchecks.utils import DeepchecksValueError
from deepchecks.metric_utils import DEFAULT_MULTICLASS_METRICS
from hamcrest import assert_that, calling, raises, close_to, starts_with, has_items

from tests.checks.utils import equal_condition_result
from tests.checks.utils import ANY_FLOAT_REGEXP


def test_dataset_wrong_input():
    bad_dataset = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(bad_dataset, None, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires dataset to be of type Dataset. instead '
                       'got: str'))


def test_model_wrong_input(iris_labeled_dataset):
    bad_model = 'wrong_input'
    # Act & Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_labeled_dataset,
                                                                    bad_model),
                raises(DeepchecksValueError,
                       'Model must inherit from one of supported models: .*'))


def test_dataset_no_label(iris_dataset):
    # Assert
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_dataset, iris_dataset, None),
                raises(DeepchecksValueError, 'Check TrainTestDifferenceOverfit requires dataset to have a '
                                           'label column'))


def test_dataset_no_shared_label(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(iris_labeled_dataset.data, label='sepal length (cm)')
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same label'))


def test_dataset_no_shared_features(iris_labeled_dataset):
    # Assert
    iris_dataset_2 = Dataset(pd.concat(
        [iris_labeled_dataset.data,
         iris_labeled_dataset.data[['sepal length (cm)']].rename(columns={'sepal length (cm)': '1'})],
        axis=1),
        label=iris_labeled_dataset.label_name())
    assert_that(calling(TrainTestDifferenceOverfit().run).with_args(iris_labeled_dataset, iris_dataset_2, None),
                raises(DeepchecksValueError,
                       'Check TrainTestDifferenceOverfit requires datasets to share the same features'))


def test_no_diff(iris_split_dataset_and_model):
    # Arrange
    train, _, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, train, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(0, 0.001))


def test_with_diff(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit()
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert_that(value, close_to(-0.035, 0.01))


def test_custom_metrics(iris_split_dataset_and_model):
    # Arrange
    train, val, model = iris_split_dataset_and_model
    check_obj = TrainTestDifferenceOverfit(
        alternative_metrics={'Accuracy': 'accuracy', 'Always 0.5': lambda x, y, z: 0.5}
    )
    result = check_obj.run(train, val, model)
    for key, value in result.value.items():
        assert_that(key, any(starts_with(metric_name) for metric_name in DEFAULT_MULTICLASS_METRICS))
        assert isinstance(value, Number)


def test_train_test_difference_condition_that_should_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_difference_not_greater_than` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
    })
    validate_train_test_condition(
        df=condition_satisfying_df,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        expecting_to_pass=True,
    )


def test_train_test_difference_condition_that_should_not_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_difference_not_greater_than` method
    """
    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # all test metrics do not satisfy condition
        'Test Metrics': {'x1': 0.5, 'x2': 0.2, 'x3': 0.3},
    })
    validate_train_test_condition(
        df=condition_unsatisfying_df,
        condition_type="train_test_difference_not_greater_than",
        condition_value=0.2,
        expecting_to_pass=False,
        expected_metrics_to_fail="x1;x2;x3"
    )


def test_train_test_ratio_condition_that_should_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_ratio_not_greater_than` method
    """
    condition_satisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        'Test Metrics': {'x1': 0.8, 'x2': 0.61, 'x3': 0.71},
    })
    validate_train_test_condition(
        df=condition_satisfying_df,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        expecting_to_pass=True,
    )


def test_train_test_ratio_condition_that_should_not_pass():
    """
    Testing condition generated by `TrainTestDifferenceOverfit.add_condition_train_test_ratio_not_greater_than` method
    """
    condition_unsatisfying_df = pd.DataFrame.from_dict({
        'Training Metrics': {'x1': 0.88, 'x2': 0.64, 'x3': 0.71},
        # all test metrics do not satisfy condition
        'Test Metrics': {'x1': 0.41, 'x2': 0.3, 'x3': 0.3},
    })
    validate_train_test_condition(
        df=condition_unsatisfying_df,
        condition_type="train_test_ratio_not_greater_than",
        condition_value=2,
        expecting_to_pass=False,
        expected_metrics_to_fail="x1;x2;x3",
    )


try:
    # NOTE: python 3.6 does not support t.Literal
    TrainDifferenceCondition = t.Union[
        t.Literal["train_test_difference_greater_than"],
        t.Literal["train_test_ratio_greater_than"]
    ]
except:
    TrainDifferenceCondition = str


def validate_train_test_condition(
    *,
    df: pd.DataFrame,
    condition_type: TrainDifferenceCondition,
    condition_value: float,
    expected_metrics_to_fail: str = "",
    expecting_to_pass: bool = True,
):
    # Check that dataframe has correct schema for the testing purpose

    if not all(df.columns == ["Training Metrics", "Test Metrics"]):
        raise ValueError("incorrect df schema")

    if not all(df["Training Metrics"].index == df["Test Metrics"].index):
        raise ValueError("incorrect df schema")
    
    check = TrainTestDifferenceOverfit()
    
    if condition_type == "train_test_ratio_not_greater_than":
        add_condition = check.add_condition_train_test_ratio_not_greater_than
        name = f'Test Train ratio is not grater than {condition_value}'
        details = f'Train Test ratio is greater than {condition_value}. Failed metrics: {expected_metrics_to_fail}'
    elif condition_type == "train_test_difference_not_greater_than":
        add_condition = check.add_condition_train_test_difference_not_greater_than
        name = f'Train Test datasets metrics difference is not greater than {condition_value}.'
        details = (
            f'Difference between Train dataset and Test dataset is greater than {condition_value}. '
            f'Failed metrics: {expected_metrics_to_fail}'
        )
    else:
        raise ValueError(f'Unknown condition type: {condition_type}')

    # NOTE: we are accessing check instance private field `_conditions`
    # in order to work with generated conditions directly

    condition, *_ = t.cast(
        t.List[Condition],
        list(add_condition(condition_value)._conditions.values())
    )

    result = t.cast(ConditionResult, condition(df))
    
    assert_that(
        result, 
        matcher=equal_condition_result( # type: ignore
            is_pass=expecting_to_pass,
            name=name,
            details=details if not expecting_to_pass else '',
            category=ConditionCategory.FAIL
        )
    )