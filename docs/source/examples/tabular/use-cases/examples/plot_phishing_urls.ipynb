{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classifying Malicious URLs\n==========================\n\nThis notebook demonstrates how the `deepchecks` package can help you\nvalidate your basic data science workflow right out of the box!\n\nThe scenario is a real business use case: You work as a data scientist\nat a cyber security startup, and the company wants to provide the\nclients with a tool to automatically detect phishing attempts performed\nthrough emails and warn clients about them. The idea is to scan emails\nand determine for each web URL they include whether it points to a\nphishing-related web page or not.\n\nSince phishing attempts are an always-adapting efforts, static black\nlists or white lists composed of good or bad URLs seen in the past are\nsimply not enough to make a good filtering system for the future. The\nway the company chose to deal with this challenge is to have you train a\nMachine Learning model to generalize what a phishing URL looks like from\nhistoric data!\n\nTo enable you to do this the company\\'s security team has collected a\nset of benign (meaning OK, or Kosher) URLs and phishing URLs observed\nduring 2019 (not necessarily in clients emails). They have also wrote a\nscript extracting features they believe should help discern phishing\nURLs from benign ones.\n\nThese features are divided to three sub-sets:\n\n-   String Characteristics - Extracted from the URL string itself.\n-   Domain Characteristics - Extracted by interacting with the domain\n    provider.\n-   Web Page Characteristics - Extracted from the content of the web\n    page the URL points to.\n\nThe string characteristics are based the way URLs are structured, and\nwhat their different parts do. Here is an informative illustration. You\ncan read more at Mozilla\\'s [What is a\nURL](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL)\narticle. We\\'ll see the specific features soon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\nfrom IPython.core.display import HTML\n\nImage(url= \"https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL/mdn-url-all.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Note: This is a slightly synthetic dataset based on [a great\nproject](https://github.com/Rohith-2/url_classification_dl) by [Rohith\nRamakrishnan](https://www.linkedin.com/in/rohith-ramakrishnan-54094a1a0/)\nand others, accompanied by a [blog\npost](https://medium.com/nerd-for-tech/url-feature-engineering-and-classification-66c0512fb34d).\nThe authors has released it under an open license per our request, and\nfor that we are very grateful to them.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Installing requirements**\n\n``` {.sourceCode .python}\nimport sys\n!{sys.executable} -m pip install deepchecks --quiet\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the data\n================\n\nOK, let\\'s take a look at the data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np; import pandas as pd; import sklearn; import deepchecks; \npd.set_option('display.max_columns', 45); SEED=832; np.random.seed(SEED);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.phishing import load_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = load_data(data_format='dataframe', as_train_test=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the actual list of features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature List\n============\n\nAnd here is a short explanation of each:\n\n  Feature Name   Feature Group              Description\n  -------------- -------------------------- -----------------------------------------------------------------------------------------\n  target         Meta Features              0 if the URL is benign, 1 if it is related to phishing\n  month          Meta Features              The month this URL was first encountered, as an int\n  scrape\\_date   Meta Features              The exact date this URL was first encountered\n  ext            String Characteristics     The domain extension\n  urlLength      String Characteristics     The number of characters in the URL\n  numDigits      String Characteristics     The number of digits in the URL\n  numParams      String Characteristics     The number of query parameters in the URL\n  [num]()%20     String Characteristics     The number of \\'%20\\' substrings in the URL\n  [num]()@       String Characteristics     The number of @ characters in the URL\n  entropy        String Characteristics     The entropy of the URL\n  has\\_ip        String Characteristics     True if the URL string contains an IP addres\n  hasHttp        Domain Characteristics     True if the url\\'s domain supports http\n  hasHttps       Domain Characteristics     True if the url\\'s domain supports https\n  urlIsLive      Domain Characteristics     The URL was live at the time of scraping\n  dsr            Domain Characteristics     The number of days since domain registration\n  dse            Domain Characteristics     The number of days since domain registration expired\n  bodyLength     Web Page Characteristics   The number of charcters in the URL\\'s web page\n  numTitles      Web Page Characteristics   The number of HTML titles (H1/H2/\\...) in the page\n  numImages      Web Page Characteristics   The number of images in the page\n  numLinks       Web Page Characteristics   The number of links in the page\n  specialChars   Web Page Characteristics   The number of special characters in the page\n  scriptLength   Web Page Characteristics   The number of charcters in scripts embedded in the page\n  sbr            Web Page Characteristics   The ratio of scriptLength to bodyLength ([= scriptLength / bodyLength]{.title-ref})\n  bscr           Web Page Characteristics   The ratio of bodyLength to specialChars ([= specialChars / bodyLength]{.title-ref})\n  sscr           Web Page Characteristics   The ratio of scriptLength to specialChars ([= scriptLength / specialChars]{.title-ref})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Integrity with Deepchecks!\n===============================\n\nThe nice thing about the `deepchecks` package is that we can already use\nit out of the box! Instead of running a single check, we use a\npre-defined test suite to run a host of data validation checks.\n\nWe think it\\'s valuable to start off with these types of suites as there\nare various issues we can identify at the get go just by looking at raw\ndata.\n\nWe will first import the appropriate factory function from the\n`deepchecks.suites` module - in this case, an integrity suite tailored\nfor a single dataset (as opposed to a division into a train and test,\nfor example) - and use it to create a new suite object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import single_dataset_integrity\ninteg_suite = single_dataset_integrity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now run that suite on our dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "integ_suite.run(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the checks\\' results!\n===================================\n\nOk, so we\\'ve got some interesting results! Even though this is quite a\ntidy dataset without even any preprocessing, `deepchecks` has found a\ncouple of columns (`has_ip` and `urlIsLive`) containing only a single\nvalue and a couple of duplicate values.\n\nWe also get a nice list of all checks that turned out ok, and what each\ncheck is about.\n\nSo nothing dramatic, but we will be sure to drop those useless columns.\n:)\n\nPreprocessing\n=============\n\nLet\\'s split the data to train and test first. Since we want to examine\nhow well a model can generalize from the past to the future, we\\'ll\nsimply assign the first months of the dataset to the training set, and\nthe last few months to the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_train_df = df[df.month <= 9]\nlen(raw_train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_test_df = df[df.month > 9]\nlen(raw_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok! Let\\'s process the data real quick and see how some baseline\nclassifiers perform!\n\nWe\\'ll just set the scrape date as our index, drop a few useless\ncolumns, one-hot encode our categorical ext column and scale all numeric\ndata:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.phishing import get_url_preprocessor\npipeline = get_url_preprocessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we\\'ll fit on and transform the raw train dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_df = pipeline.fit_transform(raw_train_df)\ntrain_X = train_df.drop('target', axis=1)\ntrain_y = train_df['target']\ntrain_X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And apply the same fitted preprocessing pipeline (with the fitted\nscaler, for example) to the test dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_df = pipeline.transform(raw_test_df)\ntest_X = test_df.drop('target', axis=1)\ntest_y = test_df['target']\ntest_X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression; from sklearn.metrics import accuracy_score; hyperparameters = {'penalty': 'l2', 'fit_intercept': True, 'random_state': SEED, 'C': 0.009}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(**hyperparameters)\nlogreg.fit(train_X, train_y);\npred_y = logreg.predict(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy_score(test_y, pred_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, so we\\'ve got a nice accuracy score from the get go! Let\\'s see what\ndeepchecks can tell us about our model\\...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import train_test_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vsuite = train_test_validation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we have to wrap the dataframes in `deepchecks.Dataset` objects to\ngive the package a bit more context, namely what is the label column,\nand whether we have a datetime column (we have, as an index, so we\\'ll\nset `set_datetime_from_dataframe_index=True`), or any categorical\nfeatures (we have none after one-hot encoding them, so we\\'ll set\n`cate_features=[]` explicitly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds_train = deepchecks.Dataset(df=train_X, label=train_y, set_datetime_from_dataframe_index=True, cat_features=[])\nds_test = deepchecks.Dataset(df=test_X, label=test_y, set_datetime_from_dataframe_index=True, cat_features=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we just have to provide the `run` method of the suite object with\nboth the model and the `Dataset` objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vsuite.run(model=logreg, train_dataset=ds_train, test_dataset=ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the checks\\' results!\n===================================\n\nWhoa! It looks like we have some time leakage!\n\nThe `Conditions` Summary section showed that the\n`Date Train-Test Leakage (overlap)` check was the only failed check. The\n`Additional Outputs` section helped us understand that the latest date\nin the train set belongs to January 2020!\n\nIt seems some entries from January 2020 made their way into the train\nset. We assumed the `month` columns was enough to split the data with\n(which it would, have all data was indeed from 2019), but as in real\nlife, things were a bit messy. We\\'ll adjust our preprocessing real\nquick, and with methodological errors out of the way we\\'ll get to\nchecking our model\\'s performance.\n\nit is also worth mentioning that deepchecks found that `urlLength` is\nthe only feature that alone can predict the target with some measure of\nsuccess. This is worth investigating!\n\nAdjusting our preprocessing and refitting the model\n---------------------------------------------------\n\nLet\\'s just drop any row from 2020 from the raw dataframe and take it\nall from there\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = df[~df['scrape_date'].str.contains('2020')]\ndf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = get_url_preprocessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_df = pipeline.fit_transform(raw_train_df)\ntrain_X = train_df.drop('target', axis=1)\ntrain_y = train_df['target']\ntrain_X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_df = pipeline.transform(raw_test_df)\ntest_X = test_df.drop('target', axis=1)\ntest_y = test_df['target']\ntest_X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logreg.fit(train_X, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred_y = logreg.predict(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy_score(test_y, pred_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deepchecks\\' Performance Checks\n===============================\n\nOk! Now that we\\'re back on track lets run some performance checks to\nsee how we did.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import model_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "msuite = model_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds_train = deepchecks.tabular.Dataset(df=train_X, label=train_y, set_datetime_from_dataframe_index=True, cat_features=[])\nds_test = deepchecks.tabular.Dataset(df=test_X, label=test_y, set_datetime_from_dataframe_index=True, cat_features=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "msuite.run(model=logreg, train_dataset=ds_train, test_dataset=ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the checks\\' results!\n===================================\n\nOk! Now that we\\'re back on track lets run some performance checks to\nsee how we did.\n\n-   `Simple Model Comparison` - This checks make sure our model\n    outperforms a very simple model to some degree. Having it fail means\n    we might have a serious problem.\n-   `Model Error Analysis` - This check analyses model errors and tries\n    to find a way to segment our data in a way that is informative to\n    error analysis. It seems that it found a valuable way to segment our\n    data, error-wise, using the `urlLength` feature. We\\'ll look into it\n    soon enough.\n-   `Trust Score Comparison`: Found a very significant decline in the\n    trust score between test and train sets. This means that test\n    samples are more likely to disagree with their counterparts (or\n    neighbours) in the train set than we would want or expect, and thus\n    our predictions on them are expected to be erroneous. (*see more in\n    the \\`paper introducing the trust score\n    \\<https://proceedings.neurips.cc/paper/2018/file/7180cffd6a8e829dacfc2a31b3f72ece-Paper.pdf\\>\\`\\_\\_*).\n\nLooking at the metric plots for F1 for both our model and a simple one\nwe see their performance are almost identical! How can this be?\nFortunately the confusion matrices automagically generated for both the\ntraining and test sets help us understand what has happened.\n\nOur evidently over-regularized classifier was over-impressed by the\nmajority class (0, or non-malicious URL), and predicted a value of 0 for\nalmost all samples in both the train and the test set, which yielded an\nseemingly-impressive 97% accuracy on the test set just due to the\nimbalanced nature of the problem.\n\n`deepchecks` also generated plots for F1, precision and recall on both\nthe train and test set, as part of the performance report, and these\nalso help us see recall scores are almost zero for both sets and\nunderstad what happaned.\n\nAdditionally, the best and worst trust score sample tables can help us\non which samples the classifier should possibly not be trusted. In this\ncase, the worst true score table is dominated by samplers with\n`target=1`, also pointing us at a problem in generalizing the notion of\nmalicisiousness from the train to the test set.\n\nTrying out a different classifier\n=================================\n\nSo let\\'s throw something a bit more rich in expressive power at the\nproblem - a decision tree!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(criterion='entropy', splitter='random', random_state=SEED)\nmodel.fit(train_X, train_y)\nmsuite.run(model=model, train_dataset=ds_train, test_dataset=ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting our model!\n===================\n\nTo try and solve the overfitting issue let\\'s try and throw at a problem\nan ensemble model that has a bit more resilience to overfitting than a\ndecision tree: a gradient-boosted ensemble of them!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(n_estimators=250, random_state=SEED, max_depth=20, subsample=0.8 , loss='exponential')\nmodel.fit(train_X, train_y)\nmsuite.run(model=model, train_dataset=ds_train, test_dataset=ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the checks\\' results!\n===================================\n\nAgain, `deepchecks` supplied some interesting insights, including a\nconsiderable performance degradation between the train and test sets. We\ncan see that the degradation in performance between the train and test\nset that we witnessed before was mitigated only very little.\n\nHowever, for a boosted model we get a pretty cool *Boosting Overfit*\ncheck that plots the accuracy of the model along increasing boosting\niterations of the model. This can help us see that we might have a minor\ncase of overfitting here, as train set accuracy is achieved rather early\non, and while test set performance improve for a little while longer,\nthey show some degradation starting from iteration 135.\n\nThis at least points to possible value in adjusting the `n_estimators`\nparameter, either reducing it or increasing it to see if degradation\ncontinues or perhaps the trends shifts.\n\nWrapping it all up!\n===================\n\nWe haven\\'t got a decent model yet, but `deepchecks` provides us with\nnumerous tools to help us navigate our development and make better\nfeature engineering and model selection decisions, by easily making\ncritical issues in data drift, overfitting, leakage, feature importance\nand model calibration readily accessible.\n\nAnd this is just what `deepchecks` can do out of the box, with the\nprebuilt checks and suites! There is a lot more potential in the way the\npackage lends itself to easy customization and creation of checks and\nsuites tailored to your needs. We will touch upon some such advanced\nuses in future guides.\n\nWe, however, hope this example can already provide you with a good\nstarting point for getting some immediate benefit out of using\ndeepchecks! Have fun, and reach out to us if you need assistance! :)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}