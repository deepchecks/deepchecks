{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single Feature Contribution Train Test\n======================================\n\nThis notebook provides an overview for using and understanding the\n\\\"Single Feature Contribution Train Test\\\" check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data](#generate-data)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe check estimates for every feature its ability to predict the label\nby itself. This check can help find:\n\n-   A potential leakage (between the label and a feature) in both\n    datasets\n    \\- e.g. due to incorrect sampling during data collection. This is a\n    critical problem, that will likely stay hidden without this check\n    (as it won\\'t pop up when comparing model performance on train and\n    test).\n\n-   A strong drift between the the feature-label relation in both\n    datasets, possibly originating from a leakage in one of the\n    datasets - e.g. a leakage that exists in the training data, but not\n    necessarily in a \\\"fresh\\\" dataset, that may have been built\n    differently.\n\nThe check is based on calculating the predictive power score (PPS) of\neach feature. For more details you can read here [how the PPS is\ncalculated](#how-is-the-predictive-power-score-pps-calculated).\n\n### What is a problematic result?\n\n1.  Features with a high predictive score - can indicate that there is a\n    leakage between the label and the feature, meaning that the feature\n    holds information that is somewhat based on the label to begin with.\n\n    For example: a bank uses their loans database to create a model of\n    whether a customer will be able to return a loan. One of the\n    features they extract is \\\"number of late payments\\\". It is clear\n    this feature will have a very strong prediction power on the\n    customer\\'s ability to return his loan, but this feature is based on\n    data the bank knows only after the loan is given, so it won\\'t be\n    available during the time of the prediction, and is a type of\n    leakage.\n\n2.  A high difference between the PPS scores of a certain feature in the\n    train and in the test datasets - this is an indication for a drift\n    between the relation of the feature and the label and a possible\n    leakage in one of the datasets.\n\n    For example: a coffee shop chain trained a model to predict the\n    number of coffee cups ordered in a store, and the model was trained\n    on data from a specific state, and tested on data from all states.\n    Running the Single Feature Contribution check on this split found\n    that there was a high difference in the PPS score of the feature\n    \\\"time\\_in\\_day\\\" - it had a much higher predictive power on the\n    training data than on the test data. Investigating this topic led to\n    detection of the problem - the time in day was saved in UTC time for\n    all states, which made the feature much less indicative for the test\n    data as it had data from several time zones (and much more coffee\n    cups are ordered in during the morning/noon than during the\n    evening/night time). This was fixed by changing the feature to be\n    the time relative to the local time zone, thus fixing its predictive\n    power and improving the model\\'s overall performance.\n\nHow is the Predictive Power Score (PPS) calculated?\n---------------------------------------------------\n\nThe features\\' predictive score results in a numeric score between 0\n(feature has no predictive power) and 1 (feature can fully predict the\nlabel alone).\n\nThe process of calculating the PPS is the following:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Extract from the data only the label and the feature being tested\n2.  Drop samples with missing values\n3.  Keep 5000 (this is configurable parameter) samples from the data\n\n4. Preprocess categorical columns. For the label using `sklearn.LabelEncoder`\n\n:   and for the feature using `sklearn.OneHotEncoder`\n\n5.  Partition the data with 4-fold cross-validation\n6.  Train decision tree\n7.  Compare the trained model\\'s performance with naive model\\'s\n    performance as follows:\n\n> Regression: The naive model always predicts the median of the label\n> column, the metric being used is MAE and the PPS calculation is:\n> $1 - \\frac{\\text{MAE model}}{\\text{MAE naive}}$ Classification: The\n> naive model always predicts the most common class of the label column,\n> The metric being used is F1 and the PPS calculation is:\n> $\\frac{\\text{F1 model} - \\text{F1 naive}}{1 - \\text{F1 naive}}$\n\nNote: all the PPS parameters can be changed by passing to the check the\nparameter `ppscore_params`\\*\n\nFor further information about PPS you can visit the [ppscore\ngithub](https://github.com/8080labs/ppscore) or the following blog post:\n[RIP correlation. Introducing the Predictive Power\nScore](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data\n=============\n\nWe\\'ll add to a given dataset a direct relation between two features and\nthe label, in order to see the Single Feature Contribution Train Test\ncheck in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.phishing import load_data\n\ndef relate_column_to_label(dataset, column, label_power):\n    col_data = dataset.data[column]\n    dataset.data[column] = col_data + (dataset.data[dataset.label_name] * col_data.mean() * label_power)\n    \ntrain_dataset, test_dataset = load_data()\n\n# Transforming 2 features in the dataset given to add correlation to the label \nrelate_column_to_label(train_dataset, 'numDigits', 10)\nrelate_column_to_label(train_dataset, 'numLinks', 10)\nrelate_column_to_label(test_dataset, 'numDigits', 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks.methodology import SingleFeatureContributionTrainTest\n\nresult = SingleFeatureContributionTrainTest().run(train_dataset=train_dataset, test_dataset=test_dataset)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nThe check shows the top features with the highest PPS difference in the\ndatasets, and elaborates how to interpret the results. By default only\nthe top 5 features are displayed, it can be changed by using the\nparameter `n_show_top` of the check.\n\nIn addition to the graphic output, the check also returns a value which\nincludes all of the information that is needed for defining the\nconditions for validation. The value is a dictionary of:\n\n-   train - for train dataset for each column the numeric PPS score (0\n    to 1)\n-   test - for test dataset for each column the numeric PPS score (0\n    to 1)\n\n\\* train-test difference - for each column the difference between the\ndatasets scores, as `train - test`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate that our pps\nscores aren\\'t too high. The check has 2 possible built-in conditions:\n`add_condition_feature_pps_difference_not_greater_than` - Validate that\nthe difference in the PPS between train and test is not larger than\ndefined amount (default 0.2)\n\n`add_condition_feature_pps_in_train_not_greater_than` - Validate that\nthe PPS scores on train dataset are not exceeding a defined amount\n(default 0.7)\n\nLet\\'s add the conditions, and re-run the check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SingleFeatureContributionTrainTest().add_condition_feature_pps_difference_not_greater_than().add_condition_feature_pps_in_train_not_greater_than()\nresult = check.run(train_dataset=train_dataset, test_dataset=test_dataset)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}