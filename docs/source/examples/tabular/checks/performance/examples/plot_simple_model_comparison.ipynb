{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple Model Comparison\n=======================\n\nThis notebooks provides an overview for using and understanding simple\nmodel comparison check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe simple model is designed to produce the best performance achievable\nusing very simple rules. The goal of the simple model is to provide a\nbaseline of minimal model performance for the given task, to which the\nuser model may be compared. If the user model achieves less or a similar\nscore to the simple model, this is an indicator for a possible problem\nwith the model (e.g. it wasn\\'t trained properly).\n\nThe check has three possible \\\"simple model\\\" heuristics, from which one\nis chosen and compared to. By default the check uses the **constant**\nheuristic, which can be overridden in the checks\\' parameters using\nsimple\\_model\\_type. There is no simple model which is more \\\"correct\\\"\nto use, each gives a different baseline to compare to, and you may\nexperiment with the different types and see how it performs on your\ndata.\n\nThe simple models are:\n\n-   A **constant** model - The default. In regression the prediction is\n    equal to the mean value, in classification the prediction is equal\n    to the most common value.\n-   A \\*\\*random88 model - Draws the prediction from the distribution of\n    the labels in the train.\n-   A **tree** model - Trains a simple decision tree with a given depth.\n    The depth can be customized using the `max_depth` parameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.phishing import load_data, load_fitted_model\n\ntrain_dataset, test_dataset = load_data()\nmodel = load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nWe will run the check with the **tree** model type. The check will use\nthe default metric defined in deepchecks for classification. This can be\noverridden by providing an alternative scorer using the\n`alternative_scorers` parameter.\n\nNote that we are demonstrating on a classification task, but the check\nalso works for regression tasks. For classification we will see the\nmetrics per class, and for regression we\\'ll have a single result per\nmetric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks.performance import SimpleModelComparison\n\n# Using tree model as a simple model, and changing the tree depth from the default 3 to 5\ncheck = SimpleModelComparison(simple_model_type='tree', max_depth=5)\ncheck.run(train_dataset, test_dataset, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe can see in the results that the check calculates the score for each\nclass in the dataset, and compares the scores between our model and the\nsimple model. In addition to the graphic output, the check also returns\na value which includes all of the information that is needed for\ndefining the conditions for validation.\n\nThe value is a dictionary of:\n\n-   scores - for each metric and class returns the numeric score\n-   type - the model task type\n-   scorers\\_perfect - for each metric the perfect possible score (used\n    to calculate gain)\n-   classes - the classes exists in the data\n\nNote: for regression `scores` will contain for each metric a single\nnumeric score, and `classes` will be null.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SimpleModelComparison()\nresult = check.run(train_dataset, test_dataset, model)\nresult.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate our model is\nbetter than the simple model by a given margin called gain. For\nclassification we check the gain for each class separately and if there\nis a class that doesn\\'t pass the defined gain the condition will fail.\n\nThe performance gain is the percent of the improved performance out of\nthe \\\"remaining\\\" unattained performance. Its purpose is to reflect the\nsignificance of the said improvement. Take for example for a metric\nbetween 0 and 1. A change of only 0.03 that takes us from 0.95 to 0.98\nis highly significant (especially in an imbalance scenario), but\nimproving from 0.1 to 0.13 is not a great achievement.\n\nThe gain is calculated as:\n$gain = \\frac{\\text{model score} - \\text{simple score}}\n{\\text{perfect score} - \\text{simple score}}$\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SimpleModelComparison(simple_model_type='tree')\ncheck.add_condition_gain_not_less_than(0.9)\nresult = check.run(train_dataset, test_dataset, model)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\%% We detected that for class \\\"1\\\" our gain did not passed the target\ngain we defined, therefore it failed.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}