{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Feature Drift\n========================\n\nThis notebooks provides an overview for using and understanding feature\ndrift check.\n\n**Structure:**\n\n-   [What is a feature drift?](#what-is-a-feature-drift)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is a feature drift?\n------------------------\n\nData drift is simply a change in the distribution of data over time. It\nis also one of the top reasons of a machine learning model performance\ndegrades over time.\n\nCauses of data drift include:\n\n-   Upstream process changes, such as a sensor being replaced that\n    changes the units of measurement from inches to centimeters.\n-   Data quality issues, such as a broken sensor always reading 0.\n-   Natural drift in the data, such as mean temperature changing with\n    the seasons.\n-   Change in relation between features, or covariate shift.\n\nFeature drift is such drift in a single feature in the dataset.\n\nIn the context of machine learning, drift between the training set and\nthe test set will likely make the model to be prone to errors. In other\nwords, this means that the model was trained on data that is different\nfrom the current test data, thus it will probably make more mistakes\npredicting the target variable.\n\n### How deepchecks detects feature drift\n\nThere are many methods to detect feature drift. Some of them include\ntraining a classifier that detects which samples come from a known\ndistribution and defines the drift by the accuracy of this classifier.\nFor more information, refer to the [Whole Dataset Drift\ncheck](/examples/tabular/checks/distribution/test_autoexamples/plot_whole_dataset_drift.html).\n\nOther approaches include statistical methods aim to measure difference\nbetween distribution of 2 given sets. We exprimented with various\napproaches and found that for detecting drift in a single feature, the\nfollowing 2 methods give the best results:\n\n-   [Population Stability Index\n    (PSI)](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)\n-   [Wasserstein metric (Earth Movers\n    Distance)](https://en.wikipedia.org/wiki/Wasserstein_metric)\n\nFor numerical features, the check uses the Earth Movers Distance method\nand for the categorical features it uses the PSI. The check calculates\ndrift between train dataset and test dataset per feature, using these 2\nstatistical measures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n\nLet\\'s generate a mock dataset of 2 categorical and 2 numerical features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\ntrain_data = np.concatenate([np.random.randn(1000,2), np.random.choice(a=['apple', 'orange', 'banana'], p=[0.5, 0.3, 0.2], size=(1000, 2))], axis=1)\ntest_data = np.concatenate([np.random.randn(1000,2), np.random.choice(a=['apple', 'orange', 'banana'], p=[0.5, 0.3, 0.2], size=(1000, 2))], axis=1)\n\ndf_train = pd.DataFrame(train_data, columns=['numeric_without_drift', 'numeric_with_drift', 'categorical_without_drift', 'categorical_with_drift'])\ndf_test = pd.DataFrame(test_data, columns=df_train.columns)\n\ndf_train = df_train.astype({'numeric_without_drift': 'float', 'numeric_with_drift': 'float'})\ndf_test = df_test.astype({'numeric_without_drift': 'float', 'numeric_with_drift': 'float'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Insert drift into test:\n=======================\n\nNow, we insert a synthetic drift into 2 columns in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_test['numeric_with_drift'] = df_test['numeric_with_drift'].astype('float') + abs(np.random.randn(1000)) + np.arange(0, 1, 0.001) * 4\ndf_test['categorical_with_drift'] = np.random.choice(a=['apple', 'orange', 'banana', 'lemon'], p=[0.5, 0.25, 0.15, 0.1], size=(1000, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training a model\n================\n\nNow, we are building a dummy model (the label is just a random numerical\ncolumn). We preprocess our synthetic dataset so categorical features are\nbeing encoded with an OrdinalEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom deepchecks.tabular import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Pipeline([\n    ('handle_cat', ColumnTransformer(\n        transformers=[\n            ('num', 'passthrough',\n             ['numeric_with_drift', 'numeric_without_drift']),\n            ('cat',\n             Pipeline([\n                 ('encode', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n             ]),\n             ['categorical_with_drift', 'categorical_without_drift'])\n        ]\n    )),\n    ('model', DecisionTreeClassifier(random_state=0, max_depth=2))]\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "label = np.random.randint(0, 2, size=(df_train.shape[0],))\ncat_features = ['categorical_without_drift', 'categorical_with_drift']\ndf_train['target'] = label\ntrain_dataset = Dataset(df_train, label='target', cat_features=cat_features)\n\nmodel.fit(train_dataset.data[train_dataset.features], label)\n\nlabel = np.random.randint(0, 2, size=(df_test.shape[0],))\ndf_test['target'] = label\ntest_dataset = Dataset(df_test, label='target', cat_features=cat_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nLet\\'s run deepchecks\\' feature drift check and see the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import TrainTestFeatureDrift\n\ncheck = TrainTestFeatureDrift()\nresult = check.run(train_dataset=train_dataset, test_dataset=test_dataset, model=model)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nAs we see from the results, the check detects and returns the drift\nscore per feature. As we expect, the features that were manually\nmanipulated to contain a strong drift in them were detected.\n\nIn addition to the graphs, each check returns a value that can be\ncontrolled in order to define expectations on that value (for example,\nto define that the drift score for every feature must be below 0.05).\n\nLet\\'s see the result value for our check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nAs we can see, we get the drift score for each feature in the dataset,\nalong with the feature importance in respect to the model.\n\nNow, we define a condition that enforce each feature\\'s drift score must\nbe below 0.1. A condition is deepchecks\\' way to enforce that results\nare OK, and we don\\'t have a problem in our data or model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_cond = check.add_condition_drift_score_not_greater_than(max_allowed_psi_score=0.2, \n                                                              max_allowed_earth_movers_score=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = check_cond.run(train_dataset=train_dataset, test_dataset=test_dataset)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see, our condition successfully detects and filters the\nproblematic features that contains a drift!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}