{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trust Score Comparison\n======================\n\nThis notebooks provides an overview for using and understanding the\ntrust score comparison check.\n\n**Structure:**\n\n-   [What is trust score?](#what-is-trust-score)\n-   [Loading the data](#loading-the-data)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is trust score?\n--------------------\n\nTrust score is an alternative measure of model confidence, used in\nclassification problems to assign a higher score to samples whose\nprediction is more likely to end up correct.\n\n### What is model confidence\n\nModel confidence commonly refers to the predicted probability of\nclassification model for the predicted class. This quantity is useful\nfor a variety of tasks:\n\n1.  Detecting \\\"problematic samples\\\" before labels become available -\n    predictions with low probability are more likely to be wrong.\n2.  Risk management - in use-cases such as loan approval, we may want to\n    weigh the probability that the loan will be returned with the loaned\n    sum and the expected return.\n3.  Early warning of concept drift - a significant decline in the\n    average confidence of samples encountered in production or test data\n    indicates that the model is predicting on more and more samples on\n    which it is unsure.\n\n### Trust Score compared to predicted probability\n\n\\\"Regular\\\" model confidence is easy to compute - just use the model\\'s\n\\\"predict\\_proba\\\" function. The danger with relying on the values\nproduced by the model itself is that they are often un-calibrated -\nwhich means that predicted probabilities don\\'t correspond to the actual\npercent of correct predictions (check the [calibration\nscore](/examples/tabular/checks/performance/test_autoexamples/plot_calibration_score.html)\n). This is because the methods and loss functions used by these models\nare often not designed to produce actual probabilities. Additionally,\nmost common classification metrics (such as precision, recall, accuracy\netc.) measure only the quality of the final prediction (after threshold\nis applied to the predicted probability) and not on the probability\nitself. This reinforces the tendency to ignore the quality of the\nprobabilities themselves.\n\nTrust Score is an alternative method for scoring the\n\\\"trust-worthiness\\\" of the model predictions that is completely\nindependent of model implementation. The method and code used by the\ndeepchecks package were published in [To Trust Or Not To Trust A\nClassifier](https://arxiv.org/abs/1805.11783).\n\nTrust score has been shown to perform better than predicted probability\nin identifying correctly classified samples, and is used by the\nTrustScoreComparison check for:\n\n1.  Identifying the samples with highest (and lowest) score - which are\n    the samples most likely (and unlikely) to be correctly classified by\n    the model. This is useful for visually detecting common qualities\n    among the highest and lowest confidence samples.\n2.  Identifying a degradation between the trust score on the test data\n    when comparing it to the training data, which may indicate that the\n    model will perform worse on test compared to train and serves as a\n    method to detect concept drift. This condition is useful especially\n    for cases when the test labels are not available, such as when\n    performing inference on new and unknown data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the data\n================\n\nWe\\'ll load the scikit-learn breast cancer dataset to test out the Trust\nScore check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom deepchecks.tabular.datasets.classification.breast_cancer import load_data\nfrom deepchecks.tabular import Dataset\n\nlabel = 'target'\n\ntrain_df, test_df = load_data(data_format='Dataframe')\ntrain = Dataset(train_df, label=label)\ntest = Dataset(test_df, label=label)\n\nclf = AdaBoostClassifier()\nfeatures = train_df.drop(label, axis=1)\ntarget = train_df[label]\nclf = clf.fit(features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nNext, we\\'ll run the check on the dataset and model, modifying the\ndefault value of `min_test_samples` in order to enable us to run this\ncheck on the small dataset. In this case, we\\'ll run the check \\\"as\nis\\\", and introduce the condition in the [next\nsection](#define-a-condition).\n\nAdditional optional parameters include the maximal sample size, the\nrandom state, the number of highest and lowest Trust Score samples to\nshow and various hyperparameters controlling the trust score algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import TrustScoreComparison\n\nTrustScoreComparison(min_test_samples=100).run(train, test, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzing the output\n====================\n\nFrom here we can see that high trust score predictions are mostly\ncorrect, while the lowest trust score samples are wrong more often than\nnot and are always predicted to belong to the negative class.\n\nFurthermore, we may notice some other common characteristics, such as\nthe fact that `worst texture` and `mean texture` both seem to be lower\nin the top scoring samples, while the worst scoring samples have high\n`worst texture` and `mean texture` values, both features with high\nfeature importance for the AdaBoost model. Might it be that high texture\nsamples are getting worse predictions by the model?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.Series(index=train_df.columns[:-1] ,data=clf.feature_importances_, name='Model Feature importance').sort_values(ascending=False).to_frame().head(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nIntroducing concept drift\n-------------------------\n\nFirst, we introduce concept drift into the data by changing the relation\nbetween the worst texture and mean concave points features, both\nimportant features for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod_test_df = test_df.copy()\nnp.random.seed(0)\nsample_idx = np.random.choice(test_df.index, 80, replace=False)\nmod_test_df.loc[sample_idx, 'worst texture'] = mod_test_df.loc[sample_idx, 'target'] * (mod_test_df.loc[sample_idx, 'mean concave points'] > 0.05)\nmod_test = Dataset(mod_test_df, label=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking for decline in Trust Score\n===================================\n\nNow, we define a condition on the Trust Score check to alert us on\nsignificant degradation in the mean Trust Score of the test data\ncompared to the training data. Note that the threshold percent of\ndecline can be modified by passing a different threshold to the\ncondition (the default is 0.2, or 20% decline).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import TrustScoreComparison\n\nTrustScoreComparison(min_test_samples=100).add_condition_mean_score_percent_decline_not_greater_than(threshold=0.19).run(train, mod_test, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzing the output\n====================\n\nThe condition alerts us to the fact that the mean Trust Score has\ndeclined by \\~21%, which is more than the 10% we allowed!\n\nThe decline is also evident in the plot showing the distribution of\nTrust Scores in each dataset, in which we can see that test data has\nsignificantly more samples with Trust Score around 1 compared to\ntraining data. We can also see the distribution of the Trust Score for\nthe modified test data used here is visibly skewed to the left (low\nTrust Score) due to the introduction of concept drift into the test\ndata. The condition helps us detect this new skew. Did this skew in the\ndata really change the performance of the model?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks.performance import MultiModelPerformanceReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MultiModelPerformanceReport().run([train, train], [test, mod_test], {'unmodified test': clf, 'modified test': clf})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the MultiModelPerformanceReport we can clearly see that several\nmetrics (such as f1, and recall) have declined on the modified test\ndataset. In a use case in which labels were not available for test data,\nwe would have still known to be wary of that thanks to the condition\nraised by the Trust Score check on the modified data!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}