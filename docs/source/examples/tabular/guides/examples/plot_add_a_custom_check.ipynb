{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a Custom Check\n==================\n\nIt is possible to extend deepchecks by implementing custom checks. This\nenables you to have your own logic of metrics or validation, or even\njust to display your own graph using deepchecks\\' suite.\n\n-   [Check Structure](#check-structure)\n-   [Write a Basic Check](#write-a-basic-check)\n-   [Check Display](#check-display)\n\nCheck Structure\n---------------\n\nEach check consists of 3 main parts:\n\n-   Return Value\n-   Display\n-   Conditions\n\nThis guide will demonstrate how to implement a Check with a return value\nand display, for addding a condition see working with conditions (LINK)\n\nWrite a Basic Check\n-------------------\n\nLet\\'s implement a check for comparing the sizes of the test and the\ntrain datasets.\n\nThe first step is to create check class, which inherits from a base\ncheck class. Each base check is differed by its run method signature,\nread more about all [types](#basic-checks-types). In this case we will\nuse `TrainTestBaseCheck`, which is used to compare between the test and\nthe train datasets. After creating the basic class with the run\\_logic\nfunction we will write our check logic inside it.\n\n*Good to know: the return value of a check can be any object, a number,\ndictionary, string, etc\\...*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import TrainTestCheck, Dataset, Context\nfrom deepchecks.core import CheckResult\n\n\nclass DatasetSizeComparison(TrainTestCheck):\n    \"\"\"Check which compares the sizes of train and test datasets.\"\"\"\n    \n    def run_logic(self, context: Context) -> CheckResult:\n        ## Check logic\n        train_size = context.train.n_samples\n        test_size = context.test.n_samples\n        \n        ## Return value as check result\n        return_value = {'train_size': train_size, 'test_size': test_size}\n        return CheckResult(return_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hooray! we just implemented a custom check. Now let\\'s create two\nDatasets and try to run it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\n# We'll use dummy data for the purpose of this demonstration\ntrain_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}), label=None)\ntest_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}), label=None)\n\nresult = DatasetSizeComparison().run(train_dataset, test_dataset)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our check ran successfully but we got the print \\\"Nothing found\\\". This\nis because we haven\\'t defined to the check anything to display, so the\ndefault behavior is to print \\\"Nothing found\\\". In order to access the\nvalue that we have defined earlier we can use the \\\"value\\\" property on\nthe result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see code references for more complex checks (that can receive\nparameters etc.), check out any of your favorite checks from our API\nReference (LINK).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Display\n=============\n\nMost of the times we will want our checks to have a visual display that\nwill quickly summarize the check result. We can pass objects for display\nto the `CheckResult`. Objects for display should be of type: html\nstring, dataframe or a function that plots a graph. Let\\'s define a\ngraph that will be displayed using `matplotlib`. In order to use\n`matplotlib` we have to implement the code inside a function and not\ncall it directly in the check, this is due to architectural limitations\nof `matplotlib`.\n\n*Good to know: \\`\\`display\\`\\` can receive a single object to display or\na list of objects*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset, TrainTestCheck, Context\nfrom deepchecks.core import CheckResult\nimport matplotlib.pyplot as plt\n\nclass DatasetSizeComparison(TrainTestCheck):\n    \"\"\"Check which compares the sizes of train and test datasets.\"\"\"\n    \n    def run_logic(self, context: Context) -> CheckResult:\n        ## Check logic\n        train_size = context.train.n_samples\n        test_size = context.test.n_samples\n        \n        ## Create the check result value\n        sizes = {'Train': train_size, 'Test': test_size}\n        sizes_df_for_display =  pd.DataFrame(sizes, index=['Size'])\n        \n        ## Display function of matplotlib graph:\n        def graph_display():\n            plt.bar(sizes.keys(), sizes.values(), color='green')\n            plt.xlabel(\"Dataset\")\n            plt.ylabel(\"Size\")\n            plt.title(\"Datasets Size Comparison\")\n        \n        return CheckResult(sizes, display=[sizes_df_for_display, graph_display])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us check it out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DatasetSizeComparison().run(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voila!\n======\n\nNow we have a check that prints a graph and has a value. We can add this\ncheck to any Suite and it will run within it.\n\nThe next possible step is to implement a condition, which will allow us\nto give the check result a pass / fail mark. To do so, check out the\nfollowing guide (LINK)\n\nBase Checks Types\n=================\n\nThere are a number of different `BaseCheck` Classes to inherit from.\nEach base check is differed by the objects it requires in order to run,\nand their sole difference is the `run` method\\'s signature.\n\n  -------------------------------------------------------------------------------------------------------------------------\n  Check                      `run` Signature                                        Notes\n  -------------------------- ------------------------------------------------------ ---------------------------------------\n  `SingleDatasetBaseCheck`   `run(self, dataset, model=None)`                       When used in a suite you can choose\n                                                                                    whether to run on the test dataset, the\n                                                                                    train dataset or on both\n\n  `TrainTestBaseCheck`       `run(self, train_dataset, test_dataset, model=None)`   \n\n  `ModelOnlyBaseCheck`       `run(self, model)`                                     \n  -------------------------------------------------------------------------------------------------------------------------\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}