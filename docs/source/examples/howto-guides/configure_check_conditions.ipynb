{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470c3f6b",
   "metadata": {},
   "source": [
    "# Configure Check Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec4ed8",
   "metadata": {},
   "source": [
    "The following guide includes different options for configuring a check's condition(s):\n",
    "\n",
    "- [Add Condition](#Add-Condition)\n",
    "- [Remove / Edit a Condition](#Remove-/-Edit-a-Condition)\n",
    "- [Add a Custom Condition](#Add-a-Custom-Condition)\n",
    "- [Set Custom Condition Category](#Set-Custom-Condition-Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026760f",
   "metadata": {},
   "source": [
    "## Add Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c850e76",
   "metadata": {},
   "source": [
    "In order to add a condition to an existing check, we can use any of the pre-defined conditions for that check. The naming convention for the methods that add the condition is `add_condition_...`.\n",
    "\n",
    "If you want to create and add your custom condition logic for parsing the check's result value, see [Add a Custom Condition](#Add-a-Custom-Condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3394b",
   "metadata": {},
   "source": [
    "### Add a condition to a new check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3862684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:56.437938Z",
     "iopub.status.busy": "2021-12-20T15:07:56.437258Z",
     "iopub.status.idle": "2021-12-20T15:07:58.310591Z",
     "shell.execute_reply": "2021-12-20T15:07:58.311137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetsSizeComparison\n",
       "\tConditions:\n",
       "\t\t0: Test dataset size is not smaller than 1000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.checks import DatasetsSizeComparison\n",
    "\n",
    "check = DatasetsSizeComparison().add_condition_test_size_not_smaller_than(1000)\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ec933",
   "metadata": {},
   "source": [
    "Conditions are used mainly in the context of a Suite, and displayed in the Conditions Summary table. For example how to run in a suite you can look at [Add a Custom Condition](#Add-a-Custom-Condition) or if you would like to run the conditions outside of suite you can execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db25f8ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.315022Z",
     "iopub.status.busy": "2021-12-20T15:07:58.314388Z",
     "iopub.status.idle": "2021-12-20T15:07:58.329429Z",
     "shell.execute_reply": "2021-12-20T15:07:58.328849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_pass': False, 'details': 'Test dataset is 3', 'category': <ConditionCategory.FAIL: 'FAIL'>, 'name': 'Test dataset size is not smaller than 1000'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks import Dataset\n",
    "import pandas as pd\n",
    "# Dummy data\n",
    "train_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}))\n",
    "test_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}))\n",
    "\n",
    "condition_results = check.conditions_decision(check.run(train_dataset, test_dataset))\n",
    "condition_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6be7c3",
   "metadata": {},
   "source": [
    "### Add a condition to a check in a suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af72d0",
   "metadata": {},
   "source": [
    "If we want to add a conditon to a check within an existing suite, we should first find the Check's ID within the suite, and then add the condition to it, by running the relevant `add_condition_` method on that check's instance. See the next section to understand how to do so.\n",
    "\n",
    "The condition will then be appended to the list of conditions on that check (or be the first one if no conditions are defined), and each condition will be evaluated separately when running the suite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fb8ec",
   "metadata": {},
   "source": [
    "## Remove / Edit a Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be40708",
   "metadata": {},
   "source": [
    "Deepchecks provides different kinds of default suites, which come with pre-defined conditions. You may want to remove a condition in case it isn't needed for you, or you may want to change the condition's parameters (since conditions functions are immutable).\n",
    "\n",
    "To remove a condition, start by printing the Suite and identifing the Check's ID, and the Condition's ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7bd116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.334130Z",
     "iopub.status.busy": "2021-12-20T15:07:58.333407Z",
     "iopub.status.idle": "2021-12-20T15:07:58.341939Z",
     "shell.execute_reply": "2021-12-20T15:07:58.342413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overfit Suite: [\n",
       "\t0: TrainTestDifferenceOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t0: Train-Test metrics degradation ratio is not greater than 0.1\n",
       "\t1: BoostingOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t0: Test score decline is not greater than 5.00%\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.suites import overfit_suite\n",
    "\n",
    "suite = overfit_suite()\n",
    "suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1da74",
   "metadata": {},
   "source": [
    "After we found the IDs we can remove the Condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf589a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.350238Z",
     "iopub.status.busy": "2021-12-20T15:07:58.348747Z",
     "iopub.status.idle": "2021-12-20T15:07:58.352310Z",
     "shell.execute_reply": "2021-12-20T15:07:58.351707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overfit Suite: [\n",
       "\t0: TrainTestDifferenceOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t0: Train-Test metrics degradation ratio is not greater than 0.1\n",
       "\t1: BoostingOverfit\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access check by id\n",
    "check = suite[1]\n",
    "# Remove condition by id\n",
    "check.remove_condition(0)\n",
    "\n",
    "suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06fd53",
   "metadata": {},
   "source": [
    "Now if we want we can also re-add the Condition using the built-in methods on the check, with a different parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9488c030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.358793Z",
     "iopub.status.busy": "2021-12-20T15:07:58.357445Z",
     "iopub.status.idle": "2021-12-20T15:07:58.361014Z",
     "shell.execute_reply": "2021-12-20T15:07:58.361488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overfit Suite: [\n",
       "\t0: TrainTestDifferenceOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t0: Train-Test metrics degradation ratio is not greater than 0.1\n",
       "\t1: BoostingOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t1: Test score decline is not greater than 20.00%\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-add the condition with new parameter\n",
    "check.add_condition_test_score_percent_decline_not_greater_than(0.2)\n",
    "\n",
    "suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b434c6",
   "metadata": {},
   "source": [
    "## Add a Custom Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73168e2e",
   "metadata": {},
   "source": [
    "In order to write conditions we first have to know what value a given check produces.  \n",
    "Let's look at the check `DatasetsSizeComparison` and see it's return value in order to write a condition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1821bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.374546Z",
     "iopub.status.busy": "2021-12-20T15:07:58.373213Z",
     "iopub.status.idle": "2021-12-20T15:07:58.376153Z",
     "shell.execute_reply": "2021-12-20T15:07:58.376591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train': 9, 'Test': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.checks import DatasetsSizeComparison\n",
    "from deepchecks import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# We'll use dummy data for the purpose of this demonstration\n",
    "train_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}))\n",
    "test_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}))\n",
    "\n",
    "result = DatasetsSizeComparison().run(train_dataset, test_dataset)\n",
    "result.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08113761",
   "metadata": {},
   "source": [
    "Now we know what the return value looks like. Let's add a new condition that validates that the ratio between the train and test datasets size is inside a given range.\n",
    "To create condition we need to use the `add_condition` method of the check which accepts a condition name and a function. This function receives the value of the `CheckResult` that we saw above and should return either a boolean or a `ConditionResult` containing a boolean and optional extra info that will be displayed in the Conditions Summary table.\n",
    "\n",
    "*Note: When implementing a condition in a custom check, you may want to add a method `add_condition_x()` to allow any consumer of your check to apply the condition (possibly with given parameters). For examples look at implemented Checks' source code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ef9c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.383830Z",
     "iopub.status.busy": "2021-12-20T15:07:58.382469Z",
     "iopub.status.idle": "2021-12-20T15:07:58.384493Z",
     "shell.execute_reply": "2021-12-20T15:07:58.384934Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks import ConditionResult\n",
    "\n",
    "# Our parameters for the condition\n",
    "low_threshold = 0.4\n",
    "high_threshold = 0.6\n",
    "\n",
    "# Create the condition function\n",
    "def custom_condition(value: dict, low=low_threshold, high=high_threshold): \n",
    "    ratio = value['Test'] / value['Train']\n",
    "    if low <= ratio <= high:\n",
    "        return ConditionResult(True)\n",
    "    else:\n",
    "        # Note: if you doesn't care about the extra info, you can return directly a boolean\n",
    "        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}')\n",
    "\n",
    "# Create the condition name\n",
    "condition_name = f'Test-Train ratio is between {low_threshold} to {high_threshold}'\n",
    "\n",
    "# Create check instance with the condition \n",
    "check = DatasetsSizeComparison().add_condition(condition_name, custom_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c194fdd",
   "metadata": {},
   "source": [
    "Now we will use a Suite to demonstrate the action of the condition, since the suite runs the condition for us automatically and prints out a Conditions Summary table (for all the conditions defined on the checks within the suite):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b8606b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.390244Z",
     "iopub.status.busy": "2021-12-20T15:07:58.389039Z",
     "iopub.status.idle": "2021-12-20T15:07:58.408813Z",
     "shell.execute_reply": "2021-12-20T15:07:58.407613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h1>Suite for Condition</h1>\n",
       "    <p>The suite is composed of various checks such as: Datasets Size Comparison, etc...<br>\n",
       "    Each check may contain conditions (which results in \n",
       "    <span style=\"color: green;display:inline-block\">✓</span> /\n",
       "    <span style=\"color: red;display:inline-block\">✖</span> /\n",
       "    <span style=\"color: orange;font-weight:bold;display:inline-block\">!</span>\n",
       "    ), as well as other outputs such as plots or tables.<br>\n",
       "    Suites, checks and conditions can all be modified (see tutorial [link]).</p>\n",
       "    <hr style=\"background-color: black;border: 0 none;color: black;height: 1px;\"><h2>Conditions Summary</h2>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_25fcc_ table {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_25fcc_ thead {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_25fcc_ tbody {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_25fcc_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_25fcc_ td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_25fcc_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >Status</th>\n",
       "      <th class=\"col_heading level0 col1\" >Check</th>\n",
       "      <th class=\"col_heading level0 col2\" >Condition</th>\n",
       "      <th class=\"col_heading level0 col3\" >More Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_25fcc_row0_col0\" class=\"data row0 col0\" ><div style=\"color: red;text-align: center\">✖</div></td>\n",
       "      <td id=\"T_25fcc_row0_col1\" class=\"data row0 col1\" >Datasets Size Comparison</td>\n",
       "      <td id=\"T_25fcc_row0_col2\" class=\"data row0 col2\" >Test-Train ratio is between 0.4 to 0.6</td>\n",
       "      <td id=\"T_25fcc_row0_col3\" class=\"data row0 col3\" >Test-Train ratio is 0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr style=\"background-color: black;border: 0 none;color: black;height: 1px;\"><h2>Additional Outputs</h2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Datasets Size Comparison</h4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Verify test dataset size comparing it to the train dataset size.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b96b5_ table {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b96b5_ thead {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b96b5_ tbody {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b96b5_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b96b5_ td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b96b5_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Train</th>\n",
       "      <th class=\"col_heading level0 col1\" >Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b96b5_level0_row0\" class=\"row_heading level0 row0\" >Size</th>\n",
       "      <td id=\"T_b96b5_row0_col0\" class=\"data row0 col0\" >9</td>\n",
       "      <td id=\"T_b96b5_row0_col1\" class=\"data row0 col1\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks import Suite\n",
    "\n",
    "# Using suite to run check & condition\n",
    "suite = Suite('Suite for Condition',\n",
    "    check\n",
    ")\n",
    "\n",
    "suite.run(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0eba67",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Set Custom Condition Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdc396",
   "metadata": {},
   "source": [
    "When writing your own condition logic, you can decide to mark a condition result as either fail or warn, by passing the category to the `ConditionResult` object.\n",
    "For example we can even write condition which sets the category based on severity of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ce24a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T15:07:58.413398Z",
     "iopub.status.busy": "2021-12-20T15:07:58.412608Z",
     "iopub.status.idle": "2021-12-20T15:07:58.420844Z",
     "shell.execute_reply": "2021-12-20T15:07:58.420078Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks import ConditionResult, ConditionCategory\n",
    "\n",
    "# Our parameters for the condition\n",
    "low_threshold = 0.3\n",
    "high_threshold = 0.7\n",
    "\n",
    "# Create the condition function for check `DatasetsSizeComparison`\n",
    "def custom_condition(value: dict): \n",
    "    ratio = value['Test'] / value['Train']\n",
    "    if low_threshold <= ratio <= high_threshold:\n",
    "        return ConditionResult(True)\n",
    "    elif ratio < low_threshold:\n",
    "        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}', ConditionCategory.FAIL)\n",
    "    else:\n",
    "        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}', ConditionCategory.WARN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
