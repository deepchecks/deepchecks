{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple Model Comparison\n=======================\n\nThis notebooks provides an overview for using and understanding simple\nmodel comparison check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data an model](#generate-data-and-model)\n-   [Run the check](#run-the-check)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe simple model is designed to produce the best performance achievable\nusing very simple rules. The goal of the simple model is to provide a\nbaseline of minimal model performance for the given task, to which the\nuser model may be compared. If the user model achieves less or a similar\nscore to the simple model, this is an indicator for a possible problem\nwith the model (e.g. it wasn\\'t trained properly).\n\nIn the computer vision module, this checks applies only to\nclassification problems.\n\nThe check has four possible strategies for selecting the behavior of the\nbaseline simple model. By default the check uses the **prior** strategy,\nwhich can be overriden in the checks\\' parameters using `strategy`.\nSimiliar to the [tabular simple model comparison\ncheck](/examples/tabular/checks/performance/test_autoexamples/simple_model_comparison.html),\nthere is no simple model which is more \\\"correct\\\" to use, each gives a\ndifferent baseline to compare to, and you may experiment with the\ndifferent types and see how it performs on your data.\n\nThe available strategies are:\n\n-   **prior** (Default) - The probability vector always contains the\n    empirical class prior distribution (i.e. the class distribution\n    observed in the training set).\n-   **most\\_frequent** - The most frequent prediction is predicted. The\n    probability vector is 1 for the most frequent prediction and 0 for\n    the other predictions.\n-   **stratified** - The predictions are generated by sampling one-hot\n    vectors from a multinomial distribution parametrized by the\n    empirical class prior probabilities.\n-   **uniform** - Generates predictions uniformly at random from the\n    list of unique classes observed in y, i.e. each class has equal\n    probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data and model\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks.performance import SimpleModelComparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification import mnist\n\nmnist_model = mnist.load_model()\ntrain_ds = mnist.load_dataset(train=True, object_type='VisionData')\ntest_ds = mnist.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nWe will run the check with the prior model type. The check will use the\ndefault classification metrics - precision and recall. This can be\noverridden by providing an alternative scorer using the\n`alternative_metrics`\\` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SimpleModelComparison(strategy='stratified')\nresult = check.run(train_ds, test_ds, mnist_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe can see in the results that the check calculates the score for each\nclass in the dataset, and compares the scores between our model and the\nsimple model.\n\nIn addition to the graphic output, the check also returns a value which\nincludes all of the information that is needed for defining the\nconditions for validation.\n\nThe value is a dataframe that contains the metrics\\' values for each\nclass and dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value.sort_values(by=['Class', 'Metric']).head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}