{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Label Drift\n======================\n\nThis notebooks provides an overview for using and understanding the\nvision label drift check.\n\n**Structure:**\n\n-   [What is a label drift?](#what-is-a-label-drift)\n-   [Run check on a Classification\n    task](#run-the-check-on-a-classification-task-mnist)\n-   [Run check on an Object Detection\n    task](#run-the-check-on-an-object-detection-task-coco)\n\nWhat is a label drift?\n----------------------\n\nThe term drift (and all it\\'s derivatives) is used to describe any\nchange in the data compared to the data the model was trained on.\nSpecifically, label drift indicates changes in the label we are trying\nto predict.\n\nCauses of label drift include:\n\n-   Natural drift in the data, such as lighting (brightness) changes\n    between summer and winter.\n-   Labeling issues, such as an analyst drawing incorrect bounding boxes\n    for an object detection task.\n\nIt may be the case that the change in the distribution of the label is\ndue solely to a change in the type of samples we\\'re receiving (for\nexample, we\\'re now getting more images of bright days) or it could be\ndue to concept drift, which is a term used to describe a situation in\nwhich the relation between the data and the target has changed (for\nexample, Covid has caused less people to be outside even in bright\nimages). The latter type of drift between the training set and the test\nset will likely make the model to be prone to errors because the\nfunction that the model is trying to approximate has changed, and the\nfirst may also lead to a change in the performance due to different\npopulations of samples being present in the data or due to trying to\npredict on outlier samples for which the model was not fitted. For both\ncases, checking for drift in the label variable between the train and\ntest datasets may serve to notify us that such a phenomenon is\noccurring.\n\n### How deepchecks detects label drift\n\nThere are many methods to detect label drift, that usually include\nstatistical methods that aim to measure difference between distribution\nof 2 given label sets. We experimented with various approaches and found\nthat for detecting drift between 2 one-dimensional distribution, the\nfollowing 2 methods give the best results:\n\n-   [Population Stability Index\n    (PSI)](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)\n-   [Wasserstein metric (Earth Movers\n    Distance)](https://en.wikipedia.org/wiki/Wasserstein_metric)\n\nFor numerical features, the check uses the Earth Movers Distance method\nand for the categorical features it uses the PSI. The check calculates\ndrift between train dataset and test dataset per feature, using these 2\nstatistical measures.\n\nDifferent measurement on label\n------------------------------\n\nIn computer vision specifically, our labels may be complex, and\nmeasuring their drift is not a straightforward task. Therefore, we\ncalculate drift on different measures on labels. For now, in deepchecks,\nwe support these measurements (on which we calculate drift):\n\n-   For both classification and object detection tasks, we calculate\n    drift in the distribution of classes.\n-   For object detection tasks, we also calculate drift in the\n    distribution of bounding box areas and distribution of number of\n    bounding boxes per image.\n\nRun the check on a Classification task (MNIST)\n----------------------------------------------\n\n### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import TrainTestLabelDrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Data\n============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import load_dataset\n\ntrain_ds = load_dataset(train=True, batch_size=64, object_type='VisionData')\ntest_ds = load_dataset(train=False, batch_size=1000, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running TrainTestLabelDrift on classification\n=============================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift()\ncheck.run(train_ds, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the results\n=========================\n\nWe can see there is almost no drift between the train & test labels.\nThis means the split to train and test was good (as it is balanced and\nrandom). Let\\'s check the performance of a simple model trained on\nMNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import load_model as load_mnist_model\nfrom deepchecks.vision.checks import ClassPerformance\n\nmnist_model = load_mnist_model(pretrained=True)\nClassPerformance().run(train_ds, test_ds, mnist_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNIST with label drift\n======================\n\nNow, let\\'s try to separate the MNIST dataset in a different manner that\nwill result in a label drift, and see how it affects the performance. We\nare going to create a custom [collate\\_fn]{.title-ref}\\` in the test\ndataset, that will select samples with class 0 in a 1/10 chances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nmnist_dataloader_train = load_dataset(train=True, batch_size=64, object_type='DataLoader')\nmnist_dataloader_test = load_dataset(train=False, batch_size=1000, object_type='DataLoader')\nfull_mnist = torch.utils.data.ConcatDataset([mnist_dataloader_train.dataset, mnist_dataloader_test.dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torch.utils.data.random_split(full_mnist, [60000,10000], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inserting drift to the test set\n===============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data._utils.collate import default_collate\nimport numpy as np\n\nnp.random.seed(42)\n\ndef collate_test(batch):\n    modified_batch = []\n    for item in batch:\n        image, label = item\n        if label == 0:\n            if np.random.randint(10) == 0:\n                modified_batch.append(item)\n        else:\n            modified_batch.append(item)\n            \n    return default_collate(modified_batch)\n\nmod_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\nmod_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=collate_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import MNISTData\n\nmod_train_ds = MNISTData(mod_train_loader)\nmod_test_ds = MNISTData(mod_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift()\ncheck.run(mod_train_ds, mod_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a condition\n===============\n\nWe could also add a condition to the check to alert us to changes in the\nlabel distribution, such as the one that occurred here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift().add_condition_drift_score_not_greater_than()\ncheck.run(mod_train_ds, mod_test_ds)\n\n# As we can see, the condition alerts us to the present of drift in the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n=======\n\nWe can see the check successfully detects the (expected) drift in class\n0 distribution between the train and test sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But how does this affect the performance of the model?\n======================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ClassPerformance().run(mod_train_ds, mod_test_ds, mnist_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferring the results\n=====================\n\nWe can see the drop in the precision of class 0, which was caused by the\nclass imbalance indicated earlier by the label drift check.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check on an Object Detection task (COCO)\n================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.coco import load_dataset\n\ntrain_ds = load_dataset(train=True, object_type='VisionData')\ntest_ds = load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift()\ncheck.run(train_ds, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Label drift is detected!\n========================\n\nWe can see that the COCO128 contains a drift in the out of the box\ndataset. In addition to the label count per class, the label drift check\nfor object detection tasks include drift calculation on certain\nmeasurements, like the bounding box area and the number of bboxes per\nimage.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}