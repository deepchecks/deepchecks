{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Prediction Drift\n===========================\n\nThis notebooks provides an overview for using and understanding the\nvision prediction drift check.\n\n**Structure:**\n\n-   [What is a prediction drift?](#what-is-a-prediction-drift)\n-   [Run check on a Classification\n    task](#run-the-check-on-a-classification-task-mnist)\n-   [Run check on an Object Detection\n    task](#run-the-check-on-an-object-detection-task-coco)\n\nWhat is a prediction drift?\n---------------------------\n\nThe term drift (and all it\\'s derivatives) is used to describe any\nchange in the data compared to the data the model was trained on.\nPrediction drift refers to the case in which a change in the data\n(data/feature drift) has happened and as a result, the distribution of\nthe models\\' prediction has changed. Calculating prediction drift is\nespecially useful in cases in which labels are not available for the\ntest dataset, and so a drift in the predictions is out only indication\nthat a changed has happened in the data that actually affects model\npredictions. If labels are available, it\\'s also recommended to run the\n[Label Drift\nCheck](/examples/vision/checks/distribution/test_autoexamples/plot_train_test_label_drift.html).\n\nThere are two main causes for prediction drift:\n\n-   Change in the sample population. In this case, the underline\n    phenomenon we\\'re trying to predict behaves the same, but we\\'re not\n    getting the same types of samples. An example for that is seasonal\n    changes that cause a changes in the brightness of the images we\\'re\n    receiving for inference. In extreme, this kind of change may take us\n    into domains that the model was not trained to function in and may\n    degrade model performance. Even if that is not the case, just\n    changing the population can result in a prediction drift if the\n    different population receive different model predictions.\n-   Concept drift, which is a case in which the underline relation\n    between the data and the label has changed. That case, also of\n    greater interest to us, won\\'t necessarily result in prediction\n    drift but it may be accompanied with a multivariate change in the\n    data distribution which will affect model predictions.\n\nIn the context of machine learning, drift between the training set and\nthe test set will likely make the model to be prone to errors. In other\nwords, this means that the model was trained on data that is different\nfrom the current test data, thus it will probably make more mistakes\npredicting the target variable.\n\n### How deepchecks detects prediction drift\n\nThere are many methods to detect drift between two distributions, that\nusually include statistical methods that aim to measure difference\nbetween distribution of 2 given sets of predictions. We experimented\nwith various approaches and found that for detecting drift between 2\none-dimensional distribution, the following 2 methods give the best\nresults:\n\n-   [Population Stability Index\n    (PSI)](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)\n-   [Wasserstein metric (Earth Movers\n    Distance)](https://en.wikipedia.org/wiki/Wasserstein_metric)\n\nFor numerical features, the check uses the Earth Movers Distance method\nand for the categorical features it uses the PSI. The check calculates\ndrift between train dataset and test dataset per feature, using these 2\nstatistical measures.\n\nDifferent measurement on predictions\n------------------------------------\n\nIn computer vision specifically, our predictions may be complex, and\nmeasuring their drift is not a straightforward task (for example, object\ndetection predictions are bounding boxes). Therefore, we calculate drift\non different measures on labels. For now, in deepchecks, we support\nthese measurements (on which we calculate drift):\n\n-   For both classification and object detection tasks, we calculate\n    drift in the distribution of classes.\n-   For object detection tasks, we also calculate drift in the\n    distribution of bounding box areas and distribution of number of\n    bounding boxes per image.\n\nRun the check on a Classification task (MNIST)\n----------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import TrainTestPredictionDrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data and model:\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import load_dataset, load_model\n\ntrain_ds = load_dataset(train=True, batch_size=64, object_type='VisionData')\ntest_ds = load_dataset(train=False, batch_size=64, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running TrainTestLabelDrift on classification\n=============================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestPredictionDrift()\ncheck.run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the results\n=========================\n\nWe can see there is almost no drift between the train & test labels.\nThis means the split to train and test was good (as it is balanced and\nrandom). Let\\'s check the performance of a simple model trained on\nMNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ClassPerformance\nClassPerformance().run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNIST with label drift\n======================\n\nNow, let\\'s try to separate the MNIST dataset in a different manner that\nwill result in a prediction drift, and see how it affects the\nperformance. We are going to create a custom collate\\_fn in the test\ndataset, that will select samples with class 0 in a 1/10 chances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nmnist_dataloader_train = load_dataset(train=True, batch_size=64, object_type='DataLoader')\nmnist_dataloader_test = load_dataset(train=False, batch_size=64, object_type='DataLoader')\nfull_mnist = torch.utils.data.ConcatDataset([mnist_dataloader_train.dataset, mnist_dataloader_test.dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torch.utils.data.random_split(full_mnist, [60000,10000], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inserting drift to the test set\n===============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data._utils.collate import default_collate\nimport numpy as np\n\nnp.random.seed(42)\n\n\ndef collate_test(batch):\n    modified_batch = []\n    for item in batch:\n        image, label = item\n        if label == 0:\n            if np.random.randint(5) == 0:\n                modified_batch.append(item)\n            else:\n                modified_batch.append((image, 1))\n        else:\n            modified_batch.append(item)\n            \n    return default_collate(modified_batch)\n\nmod_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\nmod_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=collate_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import MNISTData\n\nmod_train_ds = MNISTData(mod_train_loader)\nmod_test_ds = MNISTData(mod_test_loader)\n\n# Run the check\n# -------------\n\ncheck = TrainTestPredictionDrift()\ncheck.run(mod_train_ds, mod_test_ds, model)\n\n# Add a condition\n# ---------------\n# We could also add a condition to the check to alert us to changes in the prediction\n# distribution, such as the one that occurred here.\n\ncheck = TrainTestPredictionDrift().add_condition_drift_score_not_greater_than()\ncheck.run(mod_train_ds, mod_test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the condition alerts us to the present of drift in the\nprediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n=======\n\nWe can see the check successfully detects the (expected) drift in class\n0 distribution between the train and test sets. It means the the model\ncorrectly predicted 0 for those samples and so we\\'re seeing drift in\nthe predictions as well as the labels. We note that this check enabled\nus to detect the presence of label drift (in this case) without needing\nactual labels for the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But how does this affect the performance of the model?\n======================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ClassPerformance().run(mod_train_ds, mod_test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferring the results\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can see the drop in the precision of class 0, which was caused by the class\n# imbalance indicated earlier by the label drift check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check on an Object Detection task (COCO)\n================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.coco import load_dataset, load_model\n\ntrain_ds = load_dataset(train=True, object_type='VisionData')\ntest_ds = load_dataset(train=False, object_type='VisionData')\nmodel = load_model(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestPredictionDrift()\ncheck.run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction drift is detected!\n=============================\n\nWe can see that the COCO128 contains a drift in the out of the box\ndataset. In addition to the prediction count per class, the prediction\ndrift check for object detection tasks include drift calculation on\ncertain measurements, like the bounding box area and the number of\nbboxes per image.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}