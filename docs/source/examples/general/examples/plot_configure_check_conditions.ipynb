{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure Check Conditions\n==========================\n\nThe following guide includes different options for configuring a\ncheck\\'s condition(s):\n\n-   [Add Condition](#add-condition)\n-   [Remove / Edit a Condition](#remove-edit-a-condition)\n-   [Add a Custom Condition](#add-a-custom-condition)\n-   [Set Custom Condition Category](#set-custom-condition-category)\n\nAdd Condition\n-------------\n\nIn order to add a condition to an existing check, we can use any of the\npre-defined conditions for that check. The naming convention for the\nmethods that add the condition is `add_condition_...`.\n\nIf you want to create and add your custom condition logic for parsing\nthe check\\'s result value, see [Add a Custom\nCondition](add-a-custom-condition).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a condition to a new check\n==============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import DatasetsSizeComparison\n\ncheck = DatasetsSizeComparison().add_condition_test_size_not_smaller_than(1000)\ncheck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conditions are used mainly in the context of a Suite, and displayed in\nthe Conditions Summary table. For example how to run in a suite you can\nlook at [Add a Custom Condition](#add-a-custom-condition) or if you\nwould like to run the conditions outside of suite you can execute:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\nimport pandas as pd\n# Dummy data\ntrain_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}))\ntest_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}))\n\ncondition_results = check.conditions_decision(check.run(train_dataset, test_dataset))\ncondition_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a condition to a check in a suite\n=====================================\n\nIf we want to add a conditon to a check within an existing suite, we\nshould first find the Check\\'s ID within the suite, and then add the\ncondition to it, by running the relevant `add_condition_` method on that\ncheck\\'s instance. See the next section to understand how to do so.\n\nThe condition will then be appended to the list of conditions on that\ncheck (or be the first one if no conditions are defined), and each\ncondition will be evaluated separately when running the suite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove / Edit a Condition\n=========================\n\nDeepchecks provides different kinds of default suites, which come with\npre-defined conditions. You may want to remove a condition in case it\nisn\\'t needed for you, or you may want to change the condition\\'s\nparameters (since conditions functions are immutable).\n\nTo remove a condition, start by printing the Suite and identifing the\nCheck\\'s ID, and the Condition\\'s ID:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import train_test_leakage\n\nsuite = train_test_leakage()\nsuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we found the IDs we can remove the Condition:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Access check by id\ncheck = suite[2]\n# Remove condition by id\ncheck.remove_condition(0)\n\nsuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if we want we can also re-add the Condition using the built-in\nmethods on the check, with a different parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Re-add the condition with new parameter\ncheck.add_condition_feature_pps_difference_not_greater_than(0.01)\n\nsuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a Custom Condition\n======================\n\nIn order to write conditions we first have to know what value a given\ncheck produces.\n\nLet\\'s look at the check `DatasetsSizeComparison` and see it\\'s return\nvalue in order to write a condition for it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import DatasetsSizeComparison\nfrom deepchecks.tabular import Dataset\nimport pandas as pd\n\n# We'll use dummy data for the purpose of this demonstration\ntrain_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3,4,5,6,7,8,9]}))\ntest_dataset = Dataset(pd.DataFrame(data={'x': [1,2,3]}))\n\nresult = DatasetsSizeComparison().run(train_dataset, test_dataset)\nresult.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we know what the return value looks like. Let\\'s add a new condition\nthat validates that the ratio between the train and test datasets size\nis inside a given range. To create condition we need to use the\n`add_condition` method of the check which accepts a condition name and a\nfunction. This function receives the value of the `CheckResult` that we\nsaw above and should return either a boolean or a `ConditionResult`\ncontaining a boolean and optional extra info that will be displayed in\nthe Conditions Summary table.\n\n*Note: When implementing a condition in a custom check, you may want to\nadd a method \\`\\`add\\_condition\\_x()\\`\\` to allow any consumer of your\ncheck to apply the condition (possibly with given parameters). For\nexamples look at implemented Checks\\' source code*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.core import ConditionResult\n\n# Our parameters for the condition\nlow_threshold = 0.4\nhigh_threshold = 0.6\n\n# Create the condition function\ndef custom_condition(value: dict, low=low_threshold, high=high_threshold): \n    ratio = value['Test'] / value['Train']\n    if low <= ratio <= high:\n        return ConditionResult(True)\n    else:\n        # Note: if you doesn't care about the extra info, you can return directly a boolean\n        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}')\n\n# Create the condition name\ncondition_name = f'Test-Train ratio is between {low_threshold} to {high_threshold}'\n\n# Create check instance with the condition \ncheck = DatasetsSizeComparison().add_condition(condition_name, custom_condition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will use a Suite to demonstrate the action of the condition,\nsince the suite runs the condition for us automatically and prints out a\nConditions Summary table (for all the conditions defined on the checks\nwithin the suite):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks import Suite\n\n# Using suite to run check & condition\nsuite = Suite('Suite for Condition',\n    check\n)\n\nsuite.run(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Custom Condition Category\n=============================\n\nWhen writing your own condition logic, you can decide to mark a\ncondition result as either fail or warn, by passing the category to the\nConditionResult object. For example we can even write condition which\nsets the category based on severity of the result:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.core import ConditionResult, ConditionCategory\n\n# Our parameters for the condition\nlow_threshold = 0.3\nhigh_threshold = 0.7\n\n# Create the condition function for check `DatasetsSizeComparison`\ndef custom_condition(value: dict): \n    ratio = value['Test'] / value['Train']\n    if low_threshold <= ratio <= high_threshold:\n        return ConditionResult(True)\n    elif ratio < low_threshold:\n        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}', ConditionCategory.FAIL)\n    else:\n        return ConditionResult(False, f'Test-Train ratio is {ratio:.2}', ConditionCategory.WARN)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}